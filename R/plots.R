library(VariantAnnotation)


internal_settings_path = system.file("extdata", "test_data", "settings.json", package = "Skilift")

#' @name cov2arrowPGV
#' @description
#'
#' Prepares an scatter plot arrow file with coverage info for PGV (https://github.com/mskilab/pgv)
#'
#' @param cov input coverage data (GRanges)
#' @param field which field of the input data to use for the Y axis
#' @param output_file output file path.
#' @param ref the name of the reference to use. If not provided, then the default reference that is defined in the meta.js file will be loaded.
#' @param cov.color.field a field in the input GRanges object to use to determine the color of each point
#' @param overwrite (logical) by default, if the output path already exists, it will not be overwritten.
#' @param meta.js path to JSON file with metadata for PGV (should be located in "public/settings.json" inside the repository)
#' @param bin.width (integer) bin width for rebinning the coverage (default: 1e4)
#' @author Alon Shaiber
cov2arrowPGV = function(cov,
        field = "ratio",
        output_file = 'coverage.arrow',
        ref = 'hg19',
        cov.color.field = NULL,
        overwrite = TRUE,
        meta.js = NULL,
        ...){

    outdir = dirname(output_file)
    dir.create(outdir, showWarnings = FALSE, recursive = TRUE)

    if (!file.exists(output_file) | overwrite){
        if (!requireNamespace("arrow", quietly = TRUE)) {
            stop('You must have the package "arrow" installed in order for this function to work. Please install it.')
        }

        message('Converting coverage format')
        dat = cov2cov.js(cov, meta.js = meta.js, js.type = 'PGV', field = field,
                         ref = ref, cov.color.field = cov.color.field, ...)
        message('Done converting coverage format')

        if (!is.null(cov.color.field)){
            dat[, color := color2numeric(get(cov.color.field))]
        } else {
            if (!is.null(meta.js)){
                ref_meta = get_ref_metadata_from_PGV_json(meta.js, ref)
                setkey(ref_meta, 'chromosome')
                dat$color = color2numeric(ref_meta[dat$seqnames]$color)
            } else {
                # no cov.color.field and no meta.js so set all colors to black
                dat$color = 0
            }
        }

        outdt = dat[, .(x = new.start, y = get(field), color)]

        # if there are any NAs for colors then set those to black
        outdt[is.na(color), color := 0]

        # remove NAs
        outdt = outdt[!is.na(y)]

        # sort according to x values (that is what PGV expects)
        outdt = outdt[order(x)]

        message('Writing arrow file (using write_feather)')
        arrow_table = arrow::Table$create(outdt, schema = arrow::schema(x = arrow::float32(), y = arrow::float32(), color = arrow::float32()))
        arrow::write_feather(arrow_table, output_file, compression="uncompressed")
    } else {
        message('arrow file, "', output_file, '" already exists.')
    }
    return(output_file)
}

#' @name grab.hets
#' @title grab.hets
#'
#' @description
#'
#' returns allele gtrack given sites.txt from het pileup
#'
#' @param agt.fname (character) path to sites.txt
#' @param min.frac (numeric) between 0 and 1, min frequency in normal to count as het site
#' @param max.frac (numeric) between 0 and 1, max frequency in normal to count as het site
#' 
#' @return allele gTrack
grab.hets = function(agt.fname = NULL,
                     min.frac = 0.2,
                     max.frac = 0.8)
{
    if (is.null(agt.fname) || !file.exists(agt.fname)) {
        stop("agt.fname does not exist")
    }

    ## prepare and filter
    agt.dt = fread(agt.fname)[alt.frac.n > min.frac & alt.frac.n < max.frac,]
    ## add major and minor
    agt.dt[, which.major := ifelse(alt.count.t > ref.count.t, "alt", "ref")]
    agt.dt[, major.count := ifelse(which.major == "alt", alt.count.t, ref.count.t)]
    agt.dt[, minor.count := ifelse(which.major == "alt", ref.count.t, alt.count.t)]

    ## melt the data frame
    agt.melted = rbind(agt.dt[, .(seqnames, start, end, count = major.count, allele = "major")],
                       agt.dt[, .(seqnames, start, end, count = minor.count, allele = "minor")]
                       )

    ## make GRanges
    agt.gr = dt2gr(agt.melted[, .(seqnames, start, end, count, allele)])

    return (agt.gr)
}

#' @name subsample_hetsnps
#' @title subsample_hetsnps
#' @description subsets the hetsnps to masked unique sites and colors by major/minor allele
#'
#' @param het_pileups_wgs sites.txt from het pileup
#' @param mask rds file with masked regions
#' @param sample_size number of snps to randomly sample
#' @export
#' @author Jonathan Rafailov, Shihab Dider
subsample_hetsnps = function(
    het_pileups_wgs,
    mask=NULL,
    sample_size = 100000
) {
    if (is.null(het_pileups_wgs)) {
        stop("het_pileups_wgs does not exist")
    }
    if (is.null(mask)) {
        warning("mask does not exist, using default mask included with package.")
        maska_path = system.file("extdata", "data", "maskA_re.rds", package = "Skilift")
        maska = readRDS(maska_path)
    }

    hets.gr <- grab.hets(het_pileups_wgs)
    hets.gr = gr.val(hets.gr, maska, "mask")
    hets.gr = hets.gr %Q% (is.na(mask))
    hets.gr$mask = NULL
    # lets call major blue and minor red
    hets.gr$col <- c("major" = "red", "minor" = "blue")[hets.gr$allele]

    #lets subset a random amount of SNPS so we're under 250k points
    unique.snps = unique(gr2dt(hets.gr)[,.(seqnames, start,end)])
    n_snps = nrow(unique.snps)
    message(paste(n_snps, "snps found"))
    message(paste("subsampling", sample_size, "points..."))
    snps.to.include = unique.snps[sample(n_snps, sample_size)] %>% dt2gr()
    subset.hets.gr = hets.gr %&% snps.to.include

    return(subset.hets.gr)
}


#' @description
#' Create coverage arrow plot JSON file.
#'
#' @param plot_metadata data.table with Plot metadata, columns = (patient.id, source, x (contains path to data file or reference to data file object itself), ref, overwrite).
#' @param datadir Path to data directory with patient directories.
#' @param settings Path to settings.json file.
#'
#' @return NULL.
create_cov_arrow = function(plot_metadata, datadir, settings = internal_settings_path) {
    cov_json_path <- file.path(
        datadir,
        plot_metadata$patient.id,
        plot_metadata$source
    )

    if (!("field" %in% names(plot_metadata))) {
        stop(warning("Please include a 'field' column which indicates the column name that contains the coverage data."))
    }

    # check if the input is a GRanges or a nested list
    is_list <- is(plot_metadata$x[[1]], "list")
    if (is_list) {
        plot_metadata$x <- plot_metadata$x[[1]]
    } 

    is_granges <- is(plot_metadata$x[[1]], "GRanges")
    is_path <- is(plot_metadata$x[[1]], "character")
    if (is_granges) {
        # save granges to a temp file and assign path to plot_metadata$x
        print("Saving GRanges to temp file")
        temp_file <- tempfile(fileext = ".rds")
        saveRDS(plot_metadata$x[[1]], temp_file)
        plot_metadata$x <- temp_file
    } else if (is_path) {
        plot_metadata$x <- plot_metadata$x[[1]]
    } else {
        stop(warning("Please provide a GRanges object or a path to a GRanges object."))
    }

    if (!file.exists(cov_json_path) || plot_metadata$overwrite) {
        if (file.exists(plot_metadata$x)) {
            cov2arrowPGV(plot_metadata$x,
                field = plot_metadata$field,
                meta.js = settings,
                ref = plot_metadata$ref,
                output_file = cov_json_path
            )
        } else {
            warning(
                paste0(
                    "Input coverage file does not exist for name: ",
                    plot_metadata$sample,
                    " so no coverage will be generated."
                )
            )
        }
    } else {
        message(cov_json_path, " already exists! Set overwrite = TRUE if you want to overwrite it.")
    }
}


#' @description
#' Add gGraph JSON file to PGV datafiles.json.
#'
#' @param plot_metadata data.table with Plot metadata, columns = (patient.id, source, x (contains path to data file or reference to data file object itself), ref, overwrite).
#' @param datadir Path to data directory with patient directories.
#' @param settings Path to settings.json file.
#'
#' @return NULL.
create_ggraph_json = function(plot_metadata, datadir, settings = internal_settings_path) {
    ggraph_json_path <- file.path(
        datadir,
        plot_metadata$patient.id,
        plot_metadata$source
    )
    if (!file.exists(ggraph_json_path) || plot_metadata$overwrite) {
        if (is(plot_metadata$x[[1]], "gGraph")) {
            ggraph <- plot_metadata$x[[1]]
        } else {
            message(paste0("reading in ", plot_metadata$x))
            if (grepl(plot_metadata$x, pattern = ".rds")) {
                ggraph <- readRDS(plot_metadata$x)
            } else {
                message("Expected .rds ending for gGraph. Attempting to read anyway: ", plot_metadata$x)
                ggraph <- readRDS(plot_metadata$x)
            }
        }
        if (any(class(ggraph) == "gGraph")) {
            seq_lengths <- gGnome::parse.js.seqlengths(
                settings,
                js.type = "PGV",
                ref = plot_metadata$ref
            )
            # check for overlap in sequence names
            ggraph.reduced <- ggraph[seqnames %in% names(seq_lengths)]
            if (length(ggraph.reduced) == 0) {
                stop(sprintf(
                    'There is no overlap between the sequence names in the reference
                    used by PGV and the sequences in your gGraph. Here is an
                    example sequence from your gGraph: "%s". And here is an
                    example sequence from the reference used by gGnome.js: "%s"',
                    seqlevels(ggraph$nodes$gr)[1], names(seq_lengths)[1]
                ))
            }
            #add maxcn from plot_metadata if exists
            if("max.cn" %in% colnames(plot_metadata)) {
                maxcn = plot_metadata$max.cn
            } else {
                maxcn = 100
            }
            # sedge.id or other field
            if("col" %in% names(mcols(ggraph$nodes$gr))) { 
                nfields = "col"
            } else {
                nfields = NULL
            }
            if ("annotation" %in% colnames(plot_metadata)) {
                annotations = unlist(plot_metadata$annotation)
            } else {
                annotations = NULL
            }
            ## if ("annotation" %in% colnames(plot_metadata)) {
            # probably check for other cid.field names?
            # field = 'sedge.id'
            gGnome::refresh(ggraph[seqnames %in% names(seq_lengths)])$json(
                filename = ggraph_json_path,
                verbose = TRUE,
                annotations = annotations,
                maxcn = maxcn,
                nfields = nfields
                # cid.field = field
            )
            ## } else {
            ##     gGnome::refresh(ggraph[seqnames %in% names(seq_lengths)])$json(
            ##                                                                   filename = ggraph_json_path,
            ##                                                                   verbose = TRUE,
            ##                                                                   maxcn = maxcn,
            ##                                                                   nfields = nfields
            ##                                                               )
            ## }
        } else {
            warning(plot_metadata$x, " rds read was not a gGraph")
        }
    } else {
        warning("file ", ggraph_json_path, "already exists. Set overwrite = TRUE if you want to overwrite it.")
    }
}

#' @description
#' Create allelic gGraph JSON file.
#'
#' @param plot_metadata data.table with Plot metadata, columns = (patient.id, source, x (contains path to data file or reference to data file object itself), ref, overwrite).
#' @param datadir Path to data directory with patient directories.
#' @param settings Path to settings.json file.
#'
#' @return NULL.
create_allelic_json = function(plot_metadata, datadir, settings = internal_settings_path) {
    ggraph_json_path <- file.path(
        datadir,
        plot_metadata$patient.id,
        plot_metadata$source
    )
    if (!file.exists(ggraph_json_path) || plot_metadata$overwrite) {
        if (is(plot_metadata$x[[1]], "gGraph")) {
            ggraph <- plot_metadata$x[[1]]
        } else {
            message(paste0("reading in ", plot_metadata$x))
            if (grepl(plot_metadata$x, pattern = ".rds")) {
                ggraph <- readRDS(plot_metadata$x)
            } else {
                message("Expected .rds ending for gGraph. Attempting to read anyway: ", plot_metadata$x)
                ggraph <- readRDS(plot_metadata$x)
            }
        }
        if (any(class(ggraph) == "gGraph")) {
            seq_lengths <- gGnome::parse.js.seqlengths(
                settings,
                js.type = "PGV",
                ref = plot_metadata$ref
            )
            # check for overlap in sequence names
            ggraph.reduced <- ggraph[seqnames %in% names(seq_lengths)]
            if (length(ggraph.reduced) == 0) {
                stop(sprintf(
                    'There is no overlap between the sequence names in the reference
                    used by PGV and the sequences in your gGraph. Here is an
                    example sequence from your gGraph: "%s". And here is an
                    example sequence from the reference used by gGnome.js: "%s"',
                    seqlevels(ggraph$nodes$gr)[1], names(seq_lengths)[1]
                ))
            }
            #add maxcn from plot_metadata if exists
            if("max.cn" %in% colnames(plot_metadata)) {
                maxcn = plot_metadata$max.cn
            } else {
                maxcn = 100
            }
            if ("annotation" %in% colnames(plot_metadata)) {
                annotations = unlist(plot_metadata$annotation)
            } else {
                annotations = NULL
            }


            # sedge.id or other field
            ## if ("annotation" %in% colnames(plot_metadata)) {
            # probably check for other cid.field names?
            # field = 'sedge.id'
            gGnome::refresh(ggraph[seqnames %in% names(seq_lengths)])$json(
                filename = ggraph_json_path,
                verbose = TRUE,
                annotations = annotations,
                maxcn = maxcn,
                nfields = "col",
                save = TRUE,
                offset = TRUE
                # cid.field = field
            )
            ## for(x in 1:length(gg.js$intervals)) {
            ##     col1 = gg.js$intervals[[x]]$metadata$color
            ##     if(gg.js$intervals[[x]]$y != 0) {
            ##         if(col1 == "#0000FF80") {
            ##             gg.js$intervals[[x]]$y = gg.js$intervals[[x]]$y + 0.1
            ##         } else if(col1 == "#FF000080") {
            ##             gg.js$intervals[[x]]$y = gg.js$intervals[[x]]$y - 0.1
            ##         }
            ##     }
            ## }

            ## } else {
            ##     gg.js = gGnome::refresh(ggraph[seqnames %in% names(seq_lengths)])$json(
            ##                                                                   verbose = TRUE,
            ##                                                                   maxcn = maxcn,
            ##                                                                   nfields = "col",
            ##                                                                   save = FALSE
            ##                                                                   )
            ##     ## filename = ggraph_json_path,
            ## }
            ##temporary fix for adding padding to allelic graphs for major and minor- should probably be implented into ggnome
            #major : "#0000FF80"
            #minor : "#FF000080"

        } else {
            warning(plot_metadata$x, " rds read was not a gGraph")
        }
    } else {
        warning("file ", ggraph_json_path, "already exists. Set overwrite = TRUE if you want to overwrite it.")
    }
}

#' @description
#' Create gWalk JSON file
#'
#' @param plot_metadata data.table with Plot metadata, columns = (patient.id, source, x (contains path to data file or reference to data file object itself), ref, overwrite).
#' @param datadir Path to data directory with patient directories.
#' @param settings Path to settings.json file.
#'
#' @return NULL.
create_gwalk_json = function(plot_metadata, datadir, settings = internal_settings_path) {
    gwalk_json_path <- file.path(datadir, plot_metadata$patient.id, plot_metadata$source)
    if (!file.exists(gwalk_json_path) || plot_metadata$overwrite == TRUE) {
        if (is(plot_metadata$x[[1]], "gWalk")) {
            gwalk <- plot_metadata$x[[1]] %>% gGnome::refresh()
        } else {
            message(paste0("reading in ", plot_metadata$x))
            if (grepl(plot_metadata$x, pattern = ".rds")) {
                gwalk <- readRDS(plot_metadata$x) %>% gGnome::refresh()
            } else {
                message("Expected .rds ending for gWalk Attempting to read anyway: ", plot_metadata$x)
                gwalk <- readRDS(plot_metadata$x) %>% gGnome::refresh()
            }
        }

        if (gwalk$length == 0) {
            warning(sprintf("Zero walks in gWalk .rds file provided for sample %s!
                No walks json will be produced!", plot_metadata$sample))
            return(NA)
        }

        gwalk$json(
            filename = gwalk_json_path, verbose = TRUE,
            annotations = unlist(plot_metadat$annotation),
            include.graph = FALSE
        )
    } else {
        message(gwalk_json_path, "already exists! Set overwrite = TRUE if you want to overwrite it.")
    }
}

#' @description
#' Create mutations gGraph JSON file.
#'
#' @param plot_metadata data.table with Plot metadata, columns = (patient.id, source, x (contains path to data file or reference to data file object itself), ref, overwrite).
#' @param datadir Path to data directory with patient directories.
#' @param settings Path to settings.json file.
#'
#' @return NULL.
create_somatic_json = function(plot_metadata, datadir, settings = internal_settings_path) {
    somatic_json_path <- file.path(
        datadir,
        plot_metadata$patient.id,
        plot_metadata$source
    )
    if (!file.exists(somatic_json_path) || plot_metadata$overwrite) {
        if (any(class(plot_metadata$x[[1]]) == "data.table")) {
            mutations.dt <- plot_metadata$x[[1]]
        } else {
            message(paste0("reading in ", plot_metadata$x))
            if (grepl(plot_metadata$x, pattern = ".rds")) {
                mutations.dt <- as.data.table(readRDS(plot_metadata$x))
            } else {
                message("Expected .rds ending for mutations. Attempting to read anyway: ", plot_metadata$x)
                mutations.dt <- as.data.table(readRDS(plot_metadata$x))
            }
        }
        if (any(class(mutations.dt) == "data.table")) {
            seq_lengths <- gGnome::parse.js.seqlengths(
                settings,
                js.type = "PGV",
                ref = plot_metadata$ref
            )
            # check for overlap in sequence names
            mutations.reduced <- mutations.dt[seqnames %in% names(seq_lengths),]
            if (length(mutations.reduced) == 0) {
                stop(sprintf(
                    'There is no overlap between the sequence names in the reference
                    used by PGV and the sequences in your mutations. Here is an
                    example sequence from your mutations: "%s". And here is an
                    example sequence from the reference used by gGnome.js: "%s"',
                    mutations$seqnames[1], names(seq_lengths)[1]
                ))
            }
            yfield = plot_metadata$field[1]
            # coerce vaf col name to lower case to avoid col name mismatch for vaf/VAF column
            setnames(mutations.dt, old = "VAF", new = "vaf", skip_absent = TRUE)
            mutations.dt = mutations.dt[!is.na(get(yfield)),]
            mutations.dt[start == end, end := end +1]
            mutations.dt[, strand := NULL]
            mutations.dt[variant.p != "", annotation := paste0(
                "Type: ", annotation,
                "; Gene: ", gene,
                "; Variant: ", variant.c,
                "; Protein_variant: ", variant.p,
                "; VAF: ", vaf
            )]
            mutations.dt[variant.p == "", annotation := paste0(
                "Type: ",
                annotation,
                "; Gene: ",
                gene,
                "; Variant: ",
                variant.c,
                "; VAF: ",
                vaf
            )]
            dt2json_mut(
                dt = mutations.dt,
                ref = plot_metadata$ref,
                settings = settings,
                meta_data = c("gene", "feature_type", "annotation", "REF", "ALT", "variant.c", "variant.p", "vaf", "transcript_type", "impact", "rank"),
                y_col = yfield,
                file_name = somatic_json_path
            )
        } else {
            warning(plot_metadata$x, " rds read was not mutations")
        }
    } else {
        warning("file ", somatic_json_path, "already exists. Set overwrite = TRUE if you want to overwrite it.")
    }
}

#' @description
#' Create ppfit gGraph JSON file.
#'
#' @param plot_metadata data.table with Plot metadata, columns = (patient.id, source, x (contains path to data file or reference to data file object itself), ref, overwrite).
#' @param datadir Path to data directory with patient directories.
#' @param settings Path to settings.json file.
#'
#' @return NULL.
create_ppfit_genome_json = function(plot_metadata, datadir, settings = internal_settings_path) {
  ppfit_json_path <- file.path(
    datadir,
    plot_metadata$patient.id,
    plot_metadata$source
  )
  tryCatch({
    if (!file.exists(ppfit_json_path) || plot_metadata$overwrite) {
      if (is(plot_metadata$x[[1]], "list")) {
        ggraph <- plot_metadata$x[[1]]
      } else {
        message(paste0("reading in ", plot_metadata$x))
        if (grepl(plot_metadata$x, pattern = ".rds")) {
          ggraph <- readRDS(plot_metadata$x)
        } else {
          message("Expected .rds ending for gGraph. Attempting to read anyway: ", plot_metadata$x)
          ggraph <- readRDS(plot_metadata$x)
        }
      }
      if (any(class(ggraph) == "gGraph")) {
        seq_lengths <- gGnome::parse.js.seqlengths(
                                 settings,
                                 js.type = "PGV",
                                 ref = plot_metadata$ref
                               )
        colnames_check = c(
            "start_ix",
            "end_ix",
            "eslack_in",
            "eslack_out",
            "edges_in",
            "edges_out",
            "tile_id",
            "snode_id",
            "loose_left",
            "loose_right",
            "loose_cn_left",
            "loose_cn_right",
            "node_id",
            "raw_mean",
            "raw_var",
            "nbins",
            "nbins_tot",
            "nbins_nafrac",
            "wbins_nafrac",
            "wbins_ok",
            "mean",
            "bad",
            "max_na",
            "loess_var",
            "tau_sq_post",
            "post_var",
            "var",
            "sd"
        )

        if(all(colnames_check %in% names(ggraph$nodes$dt))) {
          ggraph2 = ggraph
          fields.keep = c(
              colnames_check,
              "seqnames",
              "start",
              "end",
              "strand",
              "width",
              "loose",
              "index"
          )
        } else {
          ## segstats information          
          segstats.dt = create_ppfit_json(
              jabba_gg = ggraph,
              path_obj = plot_metadata$x,
              return_table = TRUE,
              write_json = FALSE
          )
          segstats.gr = GRanges(segstats.dt, seqlengths = seq_lengths) %>% trim
          ggraph2 = gG(nodes = segstats.gr, edges = ggraph$edges$dt)
          fields.keep =names(segstats.dt) %>% grep("cn",.,invert = TRUE, value = TRUE)
        }
        ggraph2$set(y.field = "cn")
        ## check for overlap in sequence names
        ggraph.reduced <- ggraph2[seqnames %in% names(seq_lengths)]
        if (length(ggraph.reduced) == 0) {
          stop(sprintf(
            'There is no overlap between the sequence names in the reference
                    used by PGV and the sequences in your gGraph. Here is an
                    example sequence from your gGraph: "%s". And here is an
                    example sequence from the reference used by gGnome.js: "%s"',
            seqlevels(ggraph.reduced$nodes$gr)[1], names(seq_lengths)[1]
          ))
        }
                                        # sedge.id or other field
        if ("annotation" %in% colnames(plot_metadata)) {
                                        # probably check for other cid.field names?
                                        # field = 'sedge.id'
          ## gGnome::refresh(ggraph[seqnames %in% names(seq_lengths)])$json(
          gGnome::refresh(ggraph.reduced)$json(
                                            filename = ppfit_json_path,
                                            verbose = TRUE,
                                            annotations = unlist(plot_metadata$annotation),
                                            maxcn = 500,
                                            nfields = fields.keep,
                                            save = TRUE
                                        # cid.field = field
                                          )
        } else {
          ## gGnome::refresh(ggraph[seqnames %in% names(seq_lengths)])$json(
          gGnome::refresh(ggraph.reduced)$json(
                                            filename = ppfit_json_path,
                                            verbose = TRUE,
                                            maxcn = 500,
                                            nfields = fields.keep,
                                            save = TRUE
                                          )
        }
      } else {
        warning(plot_metadata$x, " rds read was not a gGraph")
      }
    } else {
      warning("file ", ppfit_json_path, "already exists. Set overwrite = TRUE if you want to overwrite it.")
    }
  }, error = function(e) {
    message("Error in creating ppfit plot for sample: ",plot_metadata$patient.id, "\n JaBbA:::segstats needs to be run or provided first")
    print(e)
  })
}

#' @name create_somatic_json
#' @title create_somatic_json
#' @description
#'
#' @param somatic_snv_cn somatic copy number data table
#' @param out_file where the json should be written
#' @param pair patient.id to be used for pgvdb or case reports
#' @param pgv_settings the settings file used for pgvdb or case reports
#' @param return_table_pgv TRUE/FALSE whether to return the data.table to add to pgvdb
#' @param meta_keep meta data to write in the json
#' @param y_col column to be used for y axis values
#' @return data.table to add to pgv or NULL, depends on return_table_pgv
#' @export
#' @author Stanley Clarke
## create_somatic_json = function(somatic_snv_cn, out_file, pair, pgv_settings, return_table_pgv = FALSE, meta_keep = NULL, y_col = "est_cn_llrm", ref = "hg19") {
##     som.dt = readRDS(somatic_snv_cn)
##     som.dt = som.dt[!is.na(get(y_col)),]
##     som.dt[start == end, end := end +1]
##     som.dt[, strand := NULL]
##     som.dt[variant.p != "",annotation := paste0("Type: ", annotation, "; Gene: ", gene, "; Variant: ",variant.c, "; Protein_variant: ", variant.p, "; VAF: ",vaf)]
##     som.dt[variant.p == "",annotation := paste0("Type: ", annotation, "; Gene: ", gene, "; Variant: ",variant.c, "; VAF: ",vaf)]
##     dt2json_mut(dt = som.dt, ref = ref,settings = pgv_settings, meta_data = meta_keep, y_col = y_col,
##                 file_name = out_file)
##     if(return_table_pgv) {
##         dt.add = data.table(patient.id = pair, type = "genome",visible = TRUE, title = "Copy Number Mutations", source = "mutations.json")
##     }
## }

#' @name dt2json_mut
#' @title dt2json_mut
#' @description
#'
#' function to create a mutation json, used in create_somatic_json
#' 
#' @param dt data.table with seqnames,start and end
#' @param patient.id patient id to be added to pgvdb or case reports
#' @param ref reference for pgv or case reports
#' @param file_name the file the json should be written to
#' @param meta_data extra information to add to the json
#' @param y_col column to be used for y axis values
#' @return NULL
#' @export
#' @author Stanley Clarke
dt2json_mut = function(dt,patient.id,ref,settings,file_name = paste(getwd(),"test.json",sep="/"), meta_data = NULL, y_col = NULL) {
    #create vector of seqlengths
    settings_data <- jsonlite::fromJSON(settings)
    chrom_lengths <- as.data.table(settings_data$coordinates$sets[[ref]])[,.(chromosome,startPoint,endPoint)]
    colnames(chrom_lengths) = c("seqnames","start","end")

    if(nrow(chrom_lengths[grepl("chr",seqnames),]) > 0) {
        chrom_lengths[!grepl("chr",seqnames), seqnames := paste0("chr",seqnames)] # weird fix because hg38_chr does not have chr on Y and M
    }
                                        #add y value specified
    if(is.null(y_col)) {
        dt$y_value = 1
    } else {
        dt[,y_value := get(y_col)]
    }
#convert to ggraph and create json
    if(nrow(chrom_lengths[grepl("chr",seqnames),]) > 0) {
        gr1 = dt2gr(dt[order(seqnames,start),]) %>% sortSeqlevels() %>% gr.chr()
    } else {
        gr1 = dt2gr(dt[order(seqnames,start),]) %>% sortSeqlevels() %>% gr.nochr()
    }
    if(any(gr1@seqinfo@seqlengths > chrom_lengths[seqnames %in% names(seqlengths(gr1))]$end)) {
        stop(paste("the seqlengths of your granges has ranges that are not contained in the seqlengths of",ref))
    }
    jab = gG(nodes=gr1)
    settings_y = list(y_axis = list(title = "copy number",
                                    visible = TRUE))
    node.dt = gr2dt(jab$nodes$gr[, c("snode.id","y_value",meta_data)])
    node.json = node.dt[, .(chromosome = seqnames, startPoint = start, endPoint = end, iid = snode.id,title=snode.id,type="interval", y = y_value, annotation = node.dt$annotation)]
    gg.js = list(intervals = node.json, connections = data.table())
    gg.js = c(list(settings = settings_y), gg.js)
    message(paste0("Writing json to ",file_name))
    jsonlite::write_json(gg.js, file_name,
                         pretty=TRUE, auto_unbox=TRUE, digits=4)
}

#' @name sigprofiler_decomposed_probs_json
#' @title sigprofiler_decomposed_probs_json
#' @description
#'
#' function to create decomposed probabilities json for case reports as adapted to sigprofiler outputs
#' 
#' @param probs path to sigprofiler decomposed probabilities
#' @param is_indel Boolean to indicate if the input matrix is for indels
#' @param data_dir path to data directory of which each subdirectory contains jsons per patient
#' @param pairs vector of samples for which to create jsons, by default = NULL, indicating process all patients
#' @param cores number of cores to parallelize with
#' @return data.table or NULL
#' @export
#' @author Johnathan Rafailov
sigprofiler_decomposed_probs_json = function(probs, is_indel, data_dir, pairs, cores = 1 ){
  fread(probs) -> decomposed.probs
###lets change first column to something more predictable
  colnames(decomposed.probs)[1] <- "samples"
### seems like _somatic is systematically ended to the end of all sample names in the beginning
  decomposed.probs[, samples := gsub("_somatic", "", samples)]
  if(!is.null(pairs)){decomposed.probs <- decomposed.probs[samples %in% pairs,]}
  decomposed.probs.per.sample <- melt(decomposed.probs,
                                      measure.vars = c(3:ncol(decomposed.probs)),
                                      variable.name = "signature",
                                      value.name = "p")
  samples <- unique(decomposed.probs.per.sample$samples)
  mclapply(samples, function(pair){
    pair_data = decomposed.probs.per.sample[samples == pair]
    if (is_indel) {
      samp_data = data.frame(
        signature = pair_data[,signature],
        insdel = pair_data[[2]],
        p = pair_data[,p]
      )
      catalog_file_name = "id_decomposed_prob.json"
    } else {
      samp_data = data.frame(
        signature = pair_data[, signature],
        tnc = pair_data[[2]],
        p = pair_data[,p]
      )
      catalog_file_name = "sbs_decomposed_prob.json"
    }
    write_json(samp_data, paste0(data_dir, "/", pair, "/", catalog_file_name))    
  }, mc.cores = cores)
}

#' @name oncotable
#' @title oncotable
#' @description
#'
#' THIS ONCOTABLE IS ADAPTED FOR SKILIFT
#' in that it will report only the putatively impacted transcript based on SnpEff outputs to remove duplicated SNV entries
#'
#' Takes as input (keyed) "tumors" (aka pairs) table which a metadata table with specific
#' columns pointing to paths corresponding to one or more of the following pipeline outputs:
#'
#' $annotated_bcf  Path to annotated.bcf file that is the primary output of SnpEff module from which TMB and basic mutation
#' descriptions are extracted along with their basic categories (these will comprising the core of the oncoplot are computed)
#' 
#' $fusions  Path to fusion.rds file that is the primary output of the Fusions modjle, from which protein coding fusions will
#' be computed for
#' 
#' $jabba_rds  Path to jabba.simple.rds output representing primary output of JaBbA module from which SCNA and overall
#' junction burden are computed
#' 
#' $complex    Path to complex.rds gGnome cached object that is the primary output of Events module, from which simple
#' and complex event burdens are computed
#' 
#' $signature_counts Path to signature_counts.txt that is the primary output of Signatures module from which SNV signature
#' counts are computed
#' 
#' The function then outputs a melted data.table of "interesting" features that can be saved and/or immediately output
#' into oncoprint.  This data.table will at the very least have fields $id $type (event type), $track, and  $source
#' populated in addition to a few other data type specific columns.
#'
#' The $source column is the name of the column of tumors from which that data was extracted, and track is a grouping
#' variable that allows separation of the various data types. 
#'
#' All the paths above can be NA or non existent, in which case a dummy row is inserted into the table so that downstream
#' applications know that data is missing for that sample. 
#'
#' @param tumors keyed data.table i.e. keyed by unique tumor id with specific columns corresponding to  paths to pipeline outputs(see description)
#' @param gencode path to gencode with just a single entry for each gene (so gencode entries for each gene are collapse to a single range). The input could be .gtf or .rds with GRanges object, or a GRanges object i.e. resulting from importing the (appropriate) GENCODE .gtf via rtracklayer, note: this input is only used in CNA to gene mapping. If nothing is provided then 'http://mskilab.com/fishHook/hg19/gencode.v19.genes.gtf' is used by default.
#' @param amp.thresh SCNA amplification threshold to call an amp as a function of ploidy (4)
#' @param del.thresh SCNA deletion threshold for (het) del as a function of ploidy (by default cn = 1 will be called del, but this allows additoinal regions in high ploidy tumors to be considered het dels)
#' @param mc.cores number of cores for multithreading
#' @param verbose logical flag 
#' @author Marcin Imielinski
#' @export
oncotable = function(tumors, gencode = 'http://mskilab.com/fishHook/hg19/gencode.v19.genes.gtf', verbose = TRUE, amp.thresh = 4, filter = 'PASS', del.thresh = 0.5, mc.cores = 1)
{
  gencode = process_gencode(gencode)

  if ('type' %in% names(mcols(gencode))){
      # This is a bit hacky. The hg38 object does not contain the "type" column so we check if it is there and only use it when it is present
      pge = gencode %Q% (type  == 'gene' & gene_type == 'protein_coding')
  } else {
      pge = gencode %Q% (gene_type == 'protein_coding')
  }

  .oncotable = function(dat, x = dat[[key(dat)]][1], pge, verbose = TRUE, amp.thresh = 2, del.thresh = 0.5, filter = 'PASS')
  {
    out = data.table()

    ## collect gene fusions
    if (!is.null(dat$fusions) && file.exists(dat[x, fusions]))
    {
      if (verbose)
        message('pulling $fusions for ', x)
      fus = readRDS(dat[x, fusions])$meta
      if (nrow(fus))
      {
        fus = fus[silent == FALSE, ][!duplicated(genes), ]
        fus[, vartype := ifelse(in.frame == TRUE, 'fusion', 'outframe_fusion')] # annotate out of frame fusions
        fus = fus[, .(gene = strsplit(genes, ',') %>% unlist, vartype = rep(vartype, sapply(strsplit(genes, ','), length)))][, id := x][, track := 'variants'][, type := vartype][, source := 'fusions']
        out = rbind(out, fus, fill = TRUE, use.names = TRUE)
      }
    } 
    else ## signal missing result
      out = rbind(out, data.table(id = x, type = NA, source = 'fusions'), fill = TRUE, use.names = TRUE)

    ## collect complex events
    if (!is.null(dat$complex) && file.exists(dat[x, complex]))
    {
      if (verbose)
        message('pulling $complex events for ', x)
      sv = readRDS(dat[x, complex])$meta$events
      if (nrow(sv))
      {
        sv = sv[, .(value = .N), by = type][, id := x][, track := ifelse(type %in% c('del', 'dup', 'invdup', 'tra', 'inv'), 'simple sv', 'complex sv')][, source := 'complex']
        out = rbind(out, sv, fill = TRUE, use.names = TRUE)
      }
    }
    else
      out = rbind(out, data.table(id = x, type = NA, source = 'complex'), fill = TRUE, use.names = TRUE)

    ## collect copy number / jabba
    if (!is.null(dat$jabba_rds) && file.exists(dat[x, jabba_rds]))
    {
      if (verbose)
        message('pulling $jabba_rds to get SCNA and purity / ploidy for ', x)
      jab = readRDS(dat[x, jabba_rds])
      out = rbind(out,
                  data.table(id = x, value = c(jab$purity, jab$ploidy), type = c('purity', 'ploidy'), track = 'pp'),
                  fill = TRUE, use.names = TRUE)

      # get the ncn data from jabba
      kag = readRDS(dat[x, gsub("jabba.simple.rds", "karyograph.rds", jabba_rds)])
      nseg = NULL
      if ('ncn' %in% names(mcols(kag$segstats))){
          nseg = kag$segstats[,c('ncn')]
      }
      scna = get_gene_ampdels_from_jabba(jab, amp.thresh = amp.thresh,
                                     del.thresh = del.thresh, pge = pge, nseg = nseg)

        if (nrow(scna))
        {
          scna[, track := 'variants'][, source := 'jabba_rds'][, vartype := 'scna']
          out = rbind(out,
                      scna[, .(id = x, value = min_cn, type, track, gene = gene_name)],
                      fill = TRUE, use.names = TRUE)
        }
    } else {
      out = rbind(out, data.table(id = x, type = NA, source = 'jabba_rds'), fill = TRUE, use.names = TRUE)
    }

    ## collect signatures
    if (!is.null(dat$signature_counts) && file.exists(dat[x, signature_counts]))
    {
      if (verbose)
        message('pulling $signature_counts for ', x)
      sig = fread(dat[x, signature_counts])
      sig = sig[, .(id = x, value = num_events, type = Signature, etiology = Etiology, frac = frac.events, track = 'signature', source = 'signature_counts')]
      out = rbind(out, sig, fill = TRUE, use.names = TRUE)
    }
    else
      out = rbind(out, data.table(id = x, type = NA, source = 'signature_counts'), fill = TRUE, use.names = TRUE)

    ## collect gene mutations
    if (!is.null(dat$annotated_bcf) && file.exists(dat[x, annotated_bcf]))
    {
      if (verbose)
        message('pulling $annotated_bcf for ', x, ' using FILTER=', filter)
      local_bcftools_path <- Sys.which("bcftools")
      local_bcftools_path <- ifelse(local_bcftools_path == "", stop("bcftools not found in the system PATH. Please install or moudule load bcftools."), local_bcftools_path)
      message("bcftools found at: ", local_bcftools_path)
      bcf = grok_bcf(dat[x, annotated_bcf], label = x, long = TRUE, filter = filter, bpath=local_bcftools_path)
      browser()
      if (verbose)
        message(length(bcf), ' variants pass filter')
      genome.size = sum(seqlengths(bcf), na.rm = TRUE)/1e6
      if (is.na(genome.size)) ## something went wrong with vcf
        genome.size = sum(seqlengths(gG(jabba = dat[x, jabba_rds])), na.rm = TRUE)/1e6
      nmut = data.table(as.character(seqnames(bcf)), start(bcf), end(bcf), bcf$REF, bcf$ALT) %>% unique %>% nrow
      mut.density = data.table(id = x, value = c(nmut, nmut/genome.size), type = c('count', 'density'),  track = 'tmb', source = 'annotated_bcf')
      out = rbind(out, mut.density, fill = TRUE, use.names = TRUE)
      keepeff = c('trunc', 'cnadel', 'cnadup', 'complexsv', 'splice', 'inframe_indel', 'fusion', 'missense', 'promoter', 'regulatory','mir')
      bcf = bcf[bcf$short %in% keepeff]
      if (verbose)
        message(length(bcf), ' variants pass keepeff')
      vars = NULL
      if (length(bcf))
      {
        bcf$variant.g = paste0(seqnames(bcf), ':', start(bcf), '-', end(bcf), ' ', bcf$REF, '>', bcf$ALT)
        vars = gr2dt(bcf)[, .(id = x, gene, vartype, variant.g, variant.p, distance, annotation, type = short, track = 'variants', source = 'annotated_bcf')]
        setkey(vars, variant.g)
        vars = vars[, .SD[1], by = variant.g]
      }
      out = rbind(out, vars, fill = TRUE, use.names = TRUE)
    }
    else
      out = rbind(out, data.table(id = x, type = NA, source = 'annotated_bcf'), fill = TRUE, use.names = TRUE)

    if (verbose)
      message('done ', x)

    return(out)
  }

  if (is.null(key(tumors)))
  {
    if (is.null(tumors$id))
      stop('Input tumors table must be keyed or have column $id')
    else
      setkey(tumors, id)
  }

  out = mclapply(tumors[[key(tumors)]], .oncotable,
                 dat = tumors, pge = pge, amp.thresh = amp.thresh, filter = filter, del.thresh = del.thresh, verbose = verbose, mc.cores = mc.cores)
  out = rbindlist(out, fill = TRUE, use.names = TRUE)

  setnames(out, 'id', key(tumors))
  return(out)
}


#' @name create_oncotable
#' @title create_oncotable
#' @description
#'
#' function to create oncotable for use with filtered_events_json
#' 
#' @param pair patient id 
#' @param annotated_bcf bcf with variant annotations (snpeff)
#' @param signature_counts output of Signatures task (optional)
#' @param jabba_simple jabba.simple.rds from JaBbA
#' @param fusions output of fusions task
#' @param events output of events task 
#' @param gencode file to gencode annotations (uses v29lift37 by default)
#' @param amp_thresh_multiplier amp.thresh for oncotable is amp_thresh_multiplier*ploidy
#' @param outdir path to directory in which to write oncotable outputs
#' @return data.table or NULL
#' @export
#' @author Shihab Dider, Joel Rosiene
create_oncotable = function(
    pair,
    annotated_bcf,
    signature_counts = NULL,
    fusions,
    jabba_simple,
    events,
    gencode = NULL,
    amp_thresh_multiplier = NULL,
    outdir
) {
    tumors = data.table(
      id = pair,
      annotated_bcf = annotated_bcf,
      signature_counts = signature_counts,
      fusions = fusions,
      jabba_rds = jabba_simple,
      complex = events
    )
    setkey(tumors, id)

    if (system("which bcftools", intern = TRUE) == "") {
      stop("bcftools is not available on the system PATH. Try `module load htslib` first or install it.")
    } else {
      message("bcftools is available.")
    }

    if (is.null(gencode)) {
        warning("path to gencode was not passed, setting to default ~/DB/GENCODE/gencode.v29lift37.annotation.nochr.rds")
        gencode = "~/DB/GENCODE/gencode.v29lift37.annotation.nochr.rds"
    }

    if (is.null(amp_thresh_multiplier)) {
        warning("amp_thres_multiplier was not passed, setting to default 1.5")
        amp_thresh_multiplier = 1.5
    }

    ploidy = readRDS(jabba_simple)$ploidy
    amp_thresh = amp_thresh_multiplier*ploidy
    message(paste("using amp.thresh of", amp_thresh))

    oncotable = oncotable(
      tumors,
      gencode = gencode,
      filter = "PASS",
      verbose = TRUE,
      amp.thresh=amp_thresh
    )

    if (!dir.exists(outdir)) {
        dir.create(outdir, recursive=TRUE)
    }

    saveRDS(oncotable, paste0(outdir, "/", "oncotable.rds"))
    fwrite(oncotable, paste0(outdir, "/", "oncotable.txt"))
}

#' @name filtered_events_json
#' @title filtered_events_json
#' @description
#'
#' function to create filtered events json for case reports
#' 
#' @param pair patient id to be added to pgvdb or case reports
#' @param oncotable oncotable task output
#' @param jabba_gg JaBbA output ggraph or complex
#' @param out_file path to write json
#' @param cgc_file path to cgc file to annotate drivers
#' @param return_table TRUE/FALSE whether to return the data.table that is used for creating the json
#' @return data.table or NULL
#' @export
#' @author Stanley Clarke, Tanubrata Dey

filtered_events_json = function(pair, oncotable, jabba_gg, out_file, cgc_file = "/gpfs/commons/groups/imielinski_lab/DB/COSMIC/v99_GRCh37/cancer_gene_census_fixed.csv", temp_fix = FALSE,return_table = FALSE) {
    ##Driver CNA windows
    ##Load details from oncotable
    ot = readRDS(oncotable)
    snvs = ot[grepl('frameshift|missense|stop|disruptive', annotation)]
    snvs = snvs[!duplicated(variant.p)]
    ##Note here probably have to crossreference these missense muts with hetdels
    #hetdel_snvs = snvs[gene %in% ot[type == 'hetdel',gene]]
                                        #possible_drivers = rbind(hetdel_snvs,homdels)
    homdels = ot[type == 'homdel']
    amps = ot[type == 'amp']
    jab = readRDS(jabba_gg)
    possible_drivers = rbind(snvs,homdels,amps)
    cgc = fread(cgc_file)
    names(cgc) = gsub(' ','.', names(cgc))
    cgc$gene = cgc$Gene.Symbol
    ## longlist = merge.data.table(possible_drivers, cgc, by = 'gene', all.x = TRUE)
    longlist = merge.data.table(possible_drivers, cgc, by = 'gene')
    res = longlist[ ,.(gene, id, type, variant.p, Name, Genome.Location, Tier, Role.in.Cancer)]
    names(res) = c("gene", "id", "type", "Variant", "Name", "Genome_Location", "Tier", "Role_in_Cancer")
                                        #add copy number to homdels
    res = res %>% unique
    if(nrow(res) > 0) {
        res[,seqnames := tstrsplit(Genome_Location,":",fixed=TRUE,keep=1)]
        res[,start := tstrsplit(Genome_Location,"-",fixed=TRUE,keep=1)]
        res[,start := tstrsplit(start,":",fixed=TRUE,keep=2)]
        res[,end := tstrsplit(Genome_Location,"-",fixed=TRUE,keep=2)]
        res.mut = res[!is.na(Variant),]
        if(nrow(res.mut) > 0) {
            res.mut[,Variant := gsub("p.","",Variant)]
        }
        res.cn = res[is.na(Variant),]
        if(nrow(res.cn) >0) {
            res.cn.gr = GRanges(res.cn)
            res.cn.gr = gr.val(res.cn.gr,jab$nodes$gr,c("cn","cn.low","cn.high"))
            res.cn.dt = as.data.table(res.cn.gr)
            res.cn.dt[!is.na(cn) & !is.na(cn.low) & !is.na(cn.high), Variant := paste0("Total CN:",round(cn,digits = 3),"; CN Minor:",round(cn.low,digits = 3),"; CN Major:",round(cn.high,digits = 3))]
            res.cn.dt[!is.na(cn) & is.na(cn.low) & is.na(cn.high), Variant := paste0("Total CN:",round(cn,digits = 3))]
            if(temp_fix) {
                res.cn.dt = res.cn.dt[!(type == "homdel" & cn != 0),]
                res.cn.dt = res.cn.dt[!(type == "amp" & cn <= 2),]
            }
            res.cn.dt[,c("cn", "cn.high", "cn.low", "width", "strand") := NULL] #make null, already added to Variant
            res.final = rbind(res.mut,res.cn.dt)
        } else {
            res.final = res.mut
            res.final[,c("seqnames", "start", "end") := NULL]
        }
        message(paste0("Writing json to ",out_file))
        write_json(res.final, out_file, pretty=TRUE)
        res.final[,sample := pair]
        if(return_table) {
            return(res.final)
        }
    }
}

#' @name strelka2counts
#' @title strelka2counts
#' @description
#' takes in a strelka vcf and returns a data.table with total count and count of variants with normal vaf greater than 0
#' 
#' @param vcf patient id to be added to pgvdb or case reports
#' @param seqnames_genome_width chromosomes to count variants in
#' @param type_return counts or dt, if counts returns counts for snv and snv count with a normal vaf greater than 0. If dt, returns the parsed vcf
#' @return data.table
#' @export
#' @author Stanley Clarke, Tanubrata Dey
strelka2counts = function(vcf, seqnames_genome_width = c(1:22,"X","Y"), type_return = "counts") {
    somatic.filtered.vcf = read.delim(vcf,header=F,comment.char='#',col.names=c("CHROM","POS","ID","REF","ALT","QUAL","FILTER","INFO","FORMAT","NORMAL","TUMOR")) %>% as.data.table
    ##strip chr
    somatic.filtered.vcf[, CHROM := gsub("chr","",CHROM)]
    ##
    sub.vcf = somatic.filtered.vcf[CHROM %in% seqnames_genome_width,]
    sub.vcf[, c("DP", "FDP", "SDP", "SUBDP", "AU", "CU", "GU", "TU","INDEL") := tstrsplit(NORMAL, ":", fixed = TRUE)]
    sub.vcf[REF == "A" & ALT == "T", c("ref_count","alt_count") := c(tstrsplit(AU, ",", fixed = TRUE, keep = 1), tstrsplit(TU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "A" & ALT == "C", c("ref_count","alt_count") := c(tstrsplit(AU, ",", fixed = TRUE, keep = 1), tstrsplit(CU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "A" & ALT == "G", c("ref_count","alt_count") := c(tstrsplit(AU, ",", fixed = TRUE, keep = 1), tstrsplit(GU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "C" & ALT == "T", c("ref_count","alt_count") := c(tstrsplit(CU, ",", fixed = TRUE, keep = 1), tstrsplit(TU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "C" & ALT == "A", c("ref_count","alt_count") := c(tstrsplit(CU, ",", fixed = TRUE, keep = 1), tstrsplit(AU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "C" & ALT == "G", c("ref_count","alt_count") := c(tstrsplit(CU, ",", fixed = TRUE, keep = 1), tstrsplit(GU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "G" & ALT == "T", c("ref_count","alt_count") := c(tstrsplit(GU, ",", fixed = TRUE, keep = 1), tstrsplit(TU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "G" & ALT == "C", c("ref_count","alt_count") := c(tstrsplit(GU, ",", fixed = TRUE, keep = 1), tstrsplit(CU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "G" & ALT == "A", c("ref_count","alt_count") := c(tstrsplit(GU, ",", fixed = TRUE, keep = 1), tstrsplit(AU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "T" & ALT == "A", c("ref_count","alt_count") := c(tstrsplit(TU, ",", fixed = TRUE, keep = 1), tstrsplit(AU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "T" & ALT == "C", c("ref_count","alt_count") := c(tstrsplit(TU, ",", fixed = TRUE, keep = 1), tstrsplit(CU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "T" & ALT == "G", c("ref_count","alt_count") := c(tstrsplit(TU, ",", fixed = TRUE, keep = 1), tstrsplit(GU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[, c("DP", "DP2", "TAR", "TIR", "TOR", "DP50", "FDP50", "SUBDP50", "BCN50") := tstrsplit(NORMAL, ":", fixed = TRUE)]
    sub.vcf[is.na(ref_count), ref_count := tstrsplit(TAR, ",", fixed = TRUE, keep = 1)]
    sub.vcf[is.na(alt_count), alt_count := tstrsplit(TIR, ",", fixed = TRUE, keep = 1)]
    sub.vcf[, ref_count := as.numeric(ref_count)]
    sub.vcf[, alt_count := as.numeric(alt_count)]
    sub.vcf[, normal_vaf := alt_count / (ref_count + alt_count)]
    ##now tumor vaf
    sub.vcf[, c("DP", "FDP", "SDP", "SUBDP", "AU", "CU", "GU", "TU","INDEL") := tstrsplit(TUMOR, ":", fixed = TRUE)]
    sub.vcf[REF == "A" & ALT == "T", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(AU, ",", fixed = TRUE, keep = 1), tstrsplit(TU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "A" & ALT == "C", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(AU, ",", fixed = TRUE, keep = 1), tstrsplit(CU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "A" & ALT == "G", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(AU, ",", fixed = TRUE, keep = 1), tstrsplit(GU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "C" & ALT == "T", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(CU, ",", fixed = TRUE, keep = 1), tstrsplit(TU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "C" & ALT == "A", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(CU, ",", fixed = TRUE, keep = 1), tstrsplit(AU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "C" & ALT == "G", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(CU, ",", fixed = TRUE, keep = 1), tstrsplit(GU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "G" & ALT == "T", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(GU, ",", fixed = TRUE, keep = 1), tstrsplit(TU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "G" & ALT == "C", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(GU, ",", fixed = TRUE, keep = 1), tstrsplit(CU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "G" & ALT == "A", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(GU, ",", fixed = TRUE, keep = 1), tstrsplit(AU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "T" & ALT == "A", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(TU, ",", fixed = TRUE, keep = 1), tstrsplit(AU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "T" & ALT == "C", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(TU, ",", fixed = TRUE, keep = 1), tstrsplit(CU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[REF == "T" & ALT == "G", c("ref_count_tumor","alt_count_tumor") := c(tstrsplit(TU, ",", fixed = TRUE, keep = 1), tstrsplit(GU, ",", fixed = TRUE, keep = 1)),]
    sub.vcf[, c("DP", "DP2", "TAR", "TIR", "TOR", "DP50", "FDP50", "SUBDP50", "BCN50") := tstrsplit(TUMOR, ":", fixed = TRUE)]
    sub.vcf[is.na(ref_count_tumor), ref_count_tumor := tstrsplit(TAR, ",", fixed = TRUE, keep = 1)]
    sub.vcf[is.na(alt_count_tumor), alt_count_tumor := tstrsplit(TIR, ",", fixed = TRUE, keep = 1)]
    sub.vcf[, ref_count_tumor := as.numeric(ref_count_tumor)]
    sub.vcf[, alt_count_tumor := as.numeric(alt_count_tumor)]
    sub.vcf[, tumor_vaf := alt_count_tumor / (ref_count_tumor + alt_count_tumor)]
    if(type_return == "counts") {
        snv_count = nrow(sub.vcf)
        snv_count_normal_vaf_greater0 = nrow(sub.vcf[normal_vaf > 0,])
        return(data.table(category = c("snv_count","snv_count_normal_vaf_greater0"),
                          counts = c(snv_count, snv_count_normal_vaf_greater0)))
    } else if (type_return == "dt") {
        return(sub.vcf)
    }
}


#' @name parse_vcf_strelka2
#' @title parse_vcf_strelka2
#' @description
#' takes in a strelka vcf and returns a data.table format of the vcf file with required fields for strelka_qc function
#' 
#' @param vcf patient id to be added to pgvdb or case reports
#' @param seqnames_genome_width chromosomes to count variants in
#' @return data.table
#' @export
#' @author Tanubrata Dey, Stanley Clarke
parse_vcf_strelka2 = function(vcf, seqnames_genome_width = c(1:22,"X","Y")) {                                                                                                          
    somatic.filtered.vcf = read.delim(vcf,header=F,comment.char='#',col.names=c("CHROM","POS","ID","REF","ALT","QUAL","FILTER","INFO","FORMAT","NORMAL","TUMOR")) %>% as.data.table
    ##strip chr                                                                                                                                                                    
    somatic.filtered.vcf[, CHROM := gsub("chr","",CHROM)]                                                                                                                          
    ## normal counts and VAF                                                                                                                                                                            
    sub.vcf = somatic.filtered.vcf[CHROM %in% seqnames_genome_width,]                                                                                                              
    sub.vcf[, somatic_EVS := as.numeric(gsub(".*SomaticEVS=([^;]+).*", "\\1", INFO))]                                                                                              
    sub.vcf[, MQ := as.numeric(gsub(".*MQ=([^;]+).*", "\\1", INFO))]                                                                                                               
    sub.vcf[, c("N_DP", "N_FDP", "N_SDP", "N_SUBDP", "N_AU", "N_CU", "N_GU", "N_TU","N_INDEL") := tstrsplit(NORMAL, ":", fixed = TRUE)]                                            
    sub.vcf[REF == "A" & ALT == "T", c("ref_count_N","alt_count_N") := c(tstrsplit(N_AU, ",", fixed = TRUE, keep = 1), tstrsplit(N_TU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "A" & ALT == "C", c("ref_count_N","alt_count_N") := c(tstrsplit(N_AU, ",", fixed = TRUE, keep = 1), tstrsplit(N_CU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "A" & ALT == "G", c("ref_count_N","alt_count_N") := c(tstrsplit(N_AU, ",", fixed = TRUE, keep = 1), tstrsplit(N_GU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "C" & ALT == "T", c("ref_count_N","alt_count_N") := c(tstrsplit(N_CU, ",", fixed = TRUE, keep = 1), tstrsplit(N_TU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "C" & ALT == "A", c("ref_count_N","alt_count_N") := c(tstrsplit(N_CU, ",", fixed = TRUE, keep = 1), tstrsplit(N_AU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "C" & ALT == "G", c("ref_count_N","alt_count_N") := c(tstrsplit(N_CU, ",", fixed = TRUE, keep = 1), tstrsplit(N_GU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "G" & ALT == "T", c("ref_count_N","alt_count_N") := c(tstrsplit(N_GU, ",", fixed = TRUE, keep = 1), tstrsplit(N_TU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "G" & ALT == "C", c("ref_count_N","alt_count_N") := c(tstrsplit(N_GU, ",", fixed = TRUE, keep = 1), tstrsplit(N_CU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "G" & ALT == "A", c("ref_count_N","alt_count_N") := c(tstrsplit(N_GU, ",", fixed = TRUE, keep = 1), tstrsplit(N_AU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "T" & ALT == "A", c("ref_count_N","alt_count_N") := c(tstrsplit(N_TU, ",", fixed = TRUE, keep = 1), tstrsplit(N_AU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "T" & ALT == "C", c("ref_count_N","alt_count_N") := c(tstrsplit(N_TU, ",", fixed = TRUE, keep = 1), tstrsplit(N_CU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "T" & ALT == "G", c("ref_count_N","alt_count_N") := c(tstrsplit(N_TU, ",", fixed = TRUE, keep = 1), tstrsplit(N_GU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[, c("N_DP", "N_DP2", "N_TAR", "N_TIR", "N_TOR", "N_DP50", "N_FDP50", "N_SUBDP50", "N_BCN50") := tstrsplit(NORMAL, ":", fixed = TRUE)]                                  
    sub.vcf[is.na(ref_count_N), ref_count_N := tstrsplit(N_TAR, ",", fixed = TRUE, keep = 1)]                                                                                      
    sub.vcf[is.na(alt_count_N), alt_count_N := tstrsplit(N_TIR, ",", fixed = TRUE, keep = 1)]                                                                                      
    sub.vcf[, ref_count_N := as.numeric(ref_count_N)]                                                                                                                              
    sub.vcf[, alt_count_N := as.numeric(alt_count_N)]                                                                                                                              
    sub.vcf[, VAF_N := alt_count_N / (ref_count_N + alt_count_N)]                                                                                                                  
    sub.vcf[, c("N_FDP","N_SDP","N_SUBDP","N_AU","N_CU","N_GU","N_TU","N_INDEL","N_DP2","N_TAR","N_TIR","N_TOR","N_DP50","N_FDP50","N_SUBDP50","N_BCN50")] = NULL                  
    ## tumor counts and VAF                                                                                                                                                       
    sub.vcf[, c("T_DP", "T_FDP", "T_SDP", "T_SUBDP", "T_AU", "T_CU", "T_GU", "T_TU","T_INDEL") := tstrsplit(TUMOR, ":", fixed = TRUE)]                                             
    sub.vcf[REF == "A" & ALT == "T", c("ref_count_T","alt_count_T") := c(tstrsplit(T_AU, ",", fixed = TRUE, keep = 1), tstrsplit(T_TU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "A" & ALT == "C", c("ref_count_T","alt_count_T") := c(tstrsplit(T_AU, ",", fixed = TRUE, keep = 1), tstrsplit(T_CU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "A" & ALT == "G", c("ref_count_T","alt_count_T") := c(tstrsplit(T_AU, ",", fixed = TRUE, keep = 1), tstrsplit(T_GU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "C" & ALT == "T", c("ref_count_T","alt_count_T") := c(tstrsplit(T_CU, ",", fixed = TRUE, keep = 1), tstrsplit(T_TU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "C" & ALT == "A", c("ref_count_T","alt_count_T") := c(tstrsplit(T_CU, ",", fixed = TRUE, keep = 1), tstrsplit(T_AU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "C" & ALT == "G", c("ref_count_T","alt_count_T") := c(tstrsplit(T_CU, ",", fixed = TRUE, keep = 1), tstrsplit(T_GU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "G" & ALT == "T", c("ref_count_T","alt_count_T") := c(tstrsplit(T_GU, ",", fixed = TRUE, keep = 1), tstrsplit(T_TU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "G" & ALT == "C", c("ref_count_T","alt_count_T") := c(tstrsplit(T_GU, ",", fixed = TRUE, keep = 1), tstrsplit(T_CU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "G" & ALT == "A", c("ref_count_T","alt_count_T") := c(tstrsplit(T_GU, ",", fixed = TRUE, keep = 1), tstrsplit(T_AU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "T" & ALT == "A", c("ref_count_T","alt_count_T") := c(tstrsplit(T_TU, ",", fixed = TRUE, keep = 1), tstrsplit(T_AU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "T" & ALT == "C", c("ref_count_T","alt_count_T") := c(tstrsplit(T_TU, ",", fixed = TRUE, keep = 1), tstrsplit(T_CU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[REF == "T" & ALT == "G", c("ref_count_T","alt_count_T") := c(tstrsplit(T_TU, ",", fixed = TRUE, keep = 1), tstrsplit(T_GU, ",", fixed = TRUE, keep = 1)),]             
    sub.vcf[, c("T_DP", "T_DP2", "T_TAR", "T_TIR", "T_TOR", "T_DP50", "T_FDP50", "T_SUBDP50", "T_BCN50") := tstrsplit(TUMOR, ":", fixed = TRUE)]                                   
    sub.vcf[is.na(ref_count_T), ref_count_T := tstrsplit(T_TAR, ",", fixed = TRUE, keep = 1)]                                                                                      
    sub.vcf[is.na(alt_count_T), alt_count_T := tstrsplit(T_TIR, ",", fixed = TRUE, keep = 1)]                                                                                      
    sub.vcf[, ref_count_T := as.numeric(ref_count_T)]                                                                                                                              
    sub.vcf[, alt_count_T := as.numeric(alt_count_T)]                                                                                                                              
    sub.vcf[, VAF_T := alt_count_T / (ref_count_T + alt_count_T)]                                                                                                                  
    sub.vcf[, c("T_FDP","T_SDP","T_SUBDP","T_AU","T_CU","T_GU","T_TU","T_INDEL","T_DP2","T_TAR","T_TIR","T_TOR","T_DP50","T_FDP50","T_SUBDP50","T_BCN50")] = NULL                  
    return(sub.vcf)                                                                                                                                                                
}


#' @name strelka_qc
#' @title strelka_qc
#' @description
#'
#' function to create json for strelka qc plotting in case reports
#' 
#' @param path to strelka2 vcf file to be used 
#' @param chromosomes to select for. default: c(1:22,"X","Y")
#' @param outfile path to write json
#' @param write_json TRUE/FALSE whether to write the json
#' @param return_table TRUE/FALSE whether to return the data
#' @return NULL
#' @export 
#' @author Tanubrata Dey
strelka_qc = function(vcf, seqnames_genome_width = c(1:22,"X","Y"), outfile, write_json = TRUE, return_table = TRUE) {
    sq = parse_vcf_strelka2(vcf, seqnames_genome_width = seqnames_genome_width) %>% dplyr::select(CHROM,POS,REF,ALT,FILTER,T_DP,N_DP, alt_count_N, alt_count_T, MQ,VAF_T, somatic_EVS)
    names(sq) = c("chromosome", "position", "reference", "alternate", "filter","tumor_depth", "normal_depth", "normal_alt_counts", "tumor_alt_counts","mapping_quality", "tumor_VAF", "somatic_EVS")
    consider_numeric = c("tumor_depth", "normal_depth", "normal_alt_counts", "tumor_alt_counts","mapping_quality", "tumor_vaf", "somatic_EVS")
    sq[, (consider_numeric) := lapply(.SD, as.numeric), .SDcols = consider_numeric]
    sq[, id := .I]
    if(write_json) {
    ##write the json
        message(paste0("Writing json to ",outfile))
        write_json(sq,outfile,pretty = TRUE)
        if(return_table) {
            return(sq)
        }

    } else {
        return(sq)
    }
}

#' @name sage_count
#' @title sage_count
#' @description
#' takes in a SAGE vcf and returns a data.table with total count and count of variants with normal vaf greater than 0
#' 
#' @param vcf_path Path to the SAGE VCF file
#' @param genome Reference genome name (e.g., "hg19", "hg38")
#' @return data.table
#' @export
#' @author Shihab Dider, Tanubrata Dey
sage_count = function(
    vcf_path,
    genome
) {
    vcf = readVcf(vcf_path, genome)

    # Filter for PASS variants
    pass_variants = rowRanges(vcf)$FILTER == "PASS"
    vcf = vcf[pass_variants, ]

    snv_count = length(vcf)

    # Extract VAF_N from the genotype (geno) slot
    geno_data = geno(vcf)
    normal = colnames(geno_data$DP)[1]
    tumor = colnames(geno_data$DP)[2]

    if (normal %in% colnames(geno_data$DP)) {
        VAF_N = as.numeric(geno_data$AF[, normal])
        snv_count_normal_vaf_greater0 = sum(VAF_N > 0, na.rm = TRUE)
        return(data.table(category = c("snv_count", "snv_count_normal_vaf_greater0"),
            counts = c(snv_count, snv_count_normal_vaf_greater0)))
    } else {
        print("Tumor only run VCF provided, no VAF_N present.")
        return(data.table(category = c("snv_count", "snv_count_normal_vaf_greater0"),
            counts = c(snv_count, NA)))
    }
}


#' @name sage_qc
#' @title sage_qc
#' @description
#'
#' function to create json for sage qc plotting in case reports
#' 
#' @param vcf_path Path to sage VCF file to be used (paired T-N/ Tumor only)
#' @param genome Reference genome name (e.g., "hg19", "hg38")
#' @param outfile Path to write JSON
#' @param write_json TRUE/FALSE whether to write the JSON
#' @param return_table TRUE/FALSE whether to return the data
#' @return NULL
#' @export 
#' @author Shihab Dider, Tanubrata Dey, Stanley Clarke
sage_qc = function(
    vcf_path,
    genome,
    outfile,
    write_json = TRUE,
    return_table = TRUE
) {
    vcf = readVcf(vcf_path, genome)

    # Filter for PASS variants
    pass_variants = rowRanges(vcf)$FILTER == "PASS"
    vcf = vcf[pass_variants, ]

    # Extract necessary information from VCF object
    chrom = as.character(seqnames(rowRanges(vcf)))
    pos = start(rowRanges(vcf))
    ref = as.character(ref(vcf))
    alt = as.character(unlist(alt(vcf)))
    filter = as.character(rowRanges(vcf)$FILTER)
    qual = as.numeric(rowRanges(vcf)$QUAL)

    # Extract depth and allele count information from the genotype (geno) slot
    geno_data = geno(vcf)
    normal = colnames(geno_data$DP)[1]
    tumor = colnames(geno_data$DP)[2]

    T_DP = as.numeric(geno_data$DP[, tumor])
    alt_count_T = sapply(geno_data$AD[, tumor], function(x) as.numeric(x[2]))  # Extract the second element for alternate allele depth
    T_ABQ = as.numeric(geno_data$ABQ[, tumor])
    VAF_T = as.numeric(geno_data$AF[, tumor])

    # Check if normal sample data exists
    if (normal %in% colnames(geno_data$DP)) {
        N_DP = as.numeric(geno_data$DP[, normal])
        alt_count_N = sapply(geno_data$AD[, normal], function(x) as.numeric(x[2]))  # Extract the second element for alternate allele depth
        VAF_N = as.numeric(geno_data$AF[, normal])

        sq = data.table(
            chromosome = chrom,
            position = pos,
            reference = ref,
            alternate = alt,
            filter = filter,
            mapping_quality = qual,
            tumor_depth = T_DP,
            normal_depth = N_DP,
            normal_alt_counts = alt_count_N,
            tumor_alt_counts = alt_count_T,
            tumor_abq = T_ABQ,
            tumor_vaf = VAF_T,
            normal_vaf = VAF_N
        )

        consider_numeric = c("tumor_depth", "normal_depth", "normal_alt_counts", "tumor_alt_counts", "tumor_abq", "tumor_vaf", "normal_vaf")
    } else {
        sq = data.table(
            chromosome = chrom,
            position = pos,
            reference = ref,
            alternate = alt,
            filter = filter,
            mapping_quality = qual,
            tumor_depth = T_DP,
            tumor_alt_counts = alt_count_T,
            tumor_abq = T_ABQ,
            tumor_vaf = VAF_T
        )

        consider_numeric = c("tumor_depth", "tumor_alt_counts", "tumor_abq", "tumor_vaf")
    }

    sq[, (consider_numeric) := lapply(.SD, as.numeric), .SDcols = consider_numeric]
    sq[, id := .I]

    if(write_json) {
        message(paste0("Writing json to ", outfile))
        write_json(sq, outfile, pretty = TRUE)
        if(return_table) {
            return(sq)
        }
    } else {
        return(sq)
    }
}

#' @name dlrs
#' @title derivative log ratio spread
#' @description
#'
#' function to get the dlrs for coverage data
#' used in meta_data_json
#' 
#' @param x foreground from dryclean
#' @return dlrs
#' @export
#' @author Joel Rosiene
dlrs = function(x) {
    nx = length(x)
    if (nx<3) {
        stop("Vector length>2 needed for computation")
    }
    tmp = embed(x,2)
    diffs = tmp[,2]-tmp[,1]
    dlrs = IQR(diffs,na.rm=TRUE)/(sqrt(2)*1.34)
    return(dlrs)
}


#' @name create_mutations_catalog_json
#' @title create_mutations_catalog_json
#' @description
#'
#' Builds the mutations catalog for case reports using the output of sigprofiler matrix generator
#' 
#' @param sig_matrix_path Path to either the SBS or ID matrix
#' @param is_indel Boolean to indicate if the matrix is for indels
#' @param output_dir directory to write the jsons, will create directory for each patient id
#' @return 
#' @export
#' @author Shihab Dider, Sukanya Panja
create_mutations_catalog_json = function(
    sig_matrix_path,
    is_indel,
    output_dir
) {
    sig_matrix = fread(sig_matrix_path)
    sig_matrix_dt = as.data.frame(sig_matrix)
    samples = colnames(sig_matrix_dt)
    for(i in 2:(ncol(sig_matrix_dt))) { # skip first column as it has the tnc
        pair = samples[i]

        if (is_indel) {
            samp_data = data.frame(
                id = 1:nrow(sig_matrix_dt),
                insdel = sig_matrix_dt[, 1],
                mutations = sig_matrix_dt[,i]
            )
            catalog_file_name = "id_mutation_catalog.json"
        } else {
            samp_data = data.frame(
                id = 1:nrow(sig_matrix_dt),
                tnc = sig_matrix_dt[, 1],
                mutations = sig_matrix_dt[,i]
            )
            catalog_file_name = "mutation_catalog.json"
        }
        pair_data <- list(pair = pair, data = samp_data)
        system(paste("mkdir -p", paste0(output_dir, "/", pair)))
        output_path = paste0(output_dir, "/", catalog_file_name)
        write_json(
            pair_data,
            output_path,
            pretty=TRUE,
            auto_unbox=TRUE
        )
    }
}

#' @name meta_data_json
#' @title meta_data_json
#' @description
#'
#' creates the meta data summary json for case reports
#' 
#' @param pair patient id to be added to pgvdb or case reports
#' @param outdir path to parent directory containing patient directories
#' @param coverage path dryclean coverage output
#' @param jabba_gg path to JaBbA output ggraph or complex
#' @param strelka2_vcf path to strelka vcf to get snv count
#' @param sage_vcf path to SAGE vcf to get snv count (if present)
#' @param tumor_type_final tumor type abbreviation of the sample
#' @param disease full length tumor type
#' @param primary_site primary site of tumor
#' @param inferred_sex sex of the patient
#' @param karyograph JaBbA outputted karygraph
#' @param signatures_pair_name pair name that appears in the sigprofiler output (sometimes different from pair)
#' @param indel_sigprofiler path to Assignment_Solution_Activities.txt, output of sigprofiler
#' @param sbs_sigprofiler path to Assignment_Solution_Activities.txt, output of sigprofiler 
#' @param sbs_deconstructSigs path to rds output from deconstructSigs
#' @param seqnames_loh chromosomes to be used to calculate LOH
#' @param seqnames_genome_width chromosomes to be used to calculate tmb
#' @param write_json TRUE/FALSE to write the json
#' @param overwrite TRUE/FALSE to overwrite the present json
#' @param return_table TRUE/FALSE to return the data.table output
#' @param make_dir TRUE/FALSE make the directory for the patient sample if it does not exists
#' @return data.table or NULL
#' @export
#' @author Stanley Clarke, Tanubrata Dey, Joel Rosiene, Shihab Dider

meta_data_json = function(
    pair,
    outdir = "./",
    genome = "hg19",
    coverage = NULL,
    jabba_gg = NULL,
    strelka2_vcf = NULL,
    sage_vcf = NULL,
    tumor_type = NULL,
    disease = NULL,
    primary_site = NULL,
    inferred_sex = NULL,
    karyograph = NULL,
    signatures_pair_name = NULL, # if different from pair
    indel_sigprofiler = NULL,
    sbs_sigprofiler = NULL,
    sbs_deconstructSigs = NULL,
    seqnames_loh = c(1:22),
    seqnames_genome_width = c(1:22,"X","Y"),
    write_json = TRUE,
    overwrite = FALSE,
    return_table = FALSE,
    make_dir = FALSE
) {
    out_file = paste0(outdir, "/", pair, "/metadata.json")
    if(!overwrite && write_json == TRUE) {
        if(file.exists(out_file)) {
            print(paste0('Output already exists! - skipping sample ', pair))
            return(NA)
        }
    }

    ## get patient directory
    split_file_path = strsplit(out_file, "/")[[1]]
    folder_path = paste0(split_file_path[1:(length(split_file_path)-1)], collapse = "/")
    ## check if directory exists
    if (!file.exists(folder_path)) {
        if (make_dir) {
            cmd = paste0("mkdir -p ", folder_path)
            print(paste0('Making directory ', folder_path))
            system(cmd)
        } else {
            print(paste0('Folder does not exist; skipping sample ', pair,". Use make_dir = TRUE to make directory"))
            return(NA)
        }
    }

    meta.dt = data.table(pair = pair)

    if(!is.null(tumor_type)) {
        meta.dt[, tumor_type := tumor_type]
    }
    if(!is.null(disease)) {
        meta.dt[, disease := disease]
    }
    if(!is.null(primary_site)) {
        meta.dt[, primary_site := primary_site]
    }
    if(!is.null(inferred_sex)) {
        meta.dt[, inferred_sex := inferred_sex]
    }
    
    ##get derivative log ratio spread
    if(!is.null(coverage)) {
        meta.dt$dlrs = dlrs(readRDS(coverage)$foreground)
    }

    ##snv counts
    if(!is.null(strelka2_vcf)) {
        snv.counts.dt = strelka2counts(strelka2_vcf, seqnames_genome_width = seqnames_genome_width)
        meta.dt$snv_count = snv.counts.dt[category == "snv_count",]$counts
        meta.dt$snv_count_normal_vaf_greater0 = snv.counts.dt[category == "snv_count_normal_vaf_greater0",]$counts
    }

    if(is.null(sage_vcf) || sage_vcf == "") {
        warning("SAGE VCF not found as input, will only consider Strelka2 downstream...")
    } else {
        print("Found SAGE vcf, will use SAGE counts in meta file over Strelka2...")
        sage.snv.counts.dt = sage_count(sage_vcf, genome=genome)
        meta.dt$snv_count = sage.snv.counts.dt[category == "snv_count",]$counts
        meta.dt$snv_count_normal_vaf_greater0 = sage.snv.counts.dt[category == "snv_count_normal_vaf_greater0",]$counts
    }

    ##sv counts
    if(!is.null(jabba_gg)) {
        ## count := junctions + (loose ends / 2)
        gg = readRDS(jabba_gg)
        meta.dt$junction_count = nrow(gg$junctions$dt[type != "REF",])
        meta.dt$loose_count = nrow(as.data.table(gg$loose)[terminal == FALSE,])
        meta.dt[,sv_count := (junction_count + (loose_count / 2))]

        #get loh
        nodes.dt = gg$nodes$dt
        nodes.dt[, seqnames := gsub("chr","",seqnames)] #strip chr
        nodes.dt = gg$nodes$dt[seqnames %in% seqnames_loh]
        totalseglen = nodes.dt$width %>% sum()

        if('cn.low' %in% names(nodes.dt)) {
            lohsegs = nodes.dt[cn.low==0,] %>% .[cn.high >0] %>% .$width %>% sum()
            loh_frc = lohsegs/totalseglen
            meta.dt[,loh_fraction := loh_frc]
            meta.dt[,loh_seglen := lohsegs]
            meta.dt[,loh_total_genome := totalseglen]
        } else {
            meta.dt$loh_fraction = 'Not Allelic Jabba'
        }

        ##add purity and ploidy
        meta.dt$purity = gg$meta$purity
        meta.dt$ploidy = gg$meta$ploidy
        ##add the total seqlengths by using the seqlengths in the jabba object
        nodes.gr = gg$nodes$gr
        seqlengths.dt = suppressWarnings(
            as.data.table(
                seqinfo(nodes.gr),
                keep.rownames = "seqnames"
            )
        ) #had to supressWarnings, says other arguments ignored
        seqlengths.dt[, seqnames := gsub("chr","",seqnames)] #strip chr
        seqlengths.dt = seqlengths.dt[seqnames %in% seqnames_genome_width,]
        meta.dt$total_genome_length = sum(seqlengths.dt$seqlengths)
    }

    ## Load beta/gamma for karyograph
    if(!is.null(karyograph)) {
        kag = readRDS(karyograph)
        meta.dt$beta = kag$beta
        meta.dt$gamma = kag$gamma
    }

    ##add tmb
    if(("snv_count" %in% names(meta.dt)) & ("total_genome_length" %in% names(meta.dt))) {
        meta.dt[,tmb := (snv_count / (as.numeric(meta.dt$total_genome_length) / 1e6))]
        meta.dt[,tmb := round(tmb, digits = 3)]
    }

    ## add signatures
    meta.dt$deconstructsigs_sbs_fraction = list()
    meta.dt$deletionInsertion = list()
    meta.dt$sigprofiler_indel_fraction = list()
    meta.dt$sigprofiler_indel_count = list()
    meta.dt$sigprofiler_sbs_fraction = list()
    meta.dt$sigprofiler_sbs_count = list()
    meta.dt$signatures = list()

    if (!is.null(signatures_pair_name)) {
        sig_sample_name = signatures_pair_name
    } else {
        sig_sample_name = pair
    }

    if(!is.null(sbs_deconstructSigs)) {
        signatures = sbs_deconstructSigs2meta(sbs_file = sbs_deconstructSigs, sample = sig_sample_name)
        meta.dt$deconstructsigs_sbs_fraction = list(as.list(signatures))
    }

    if(!is.null(indel_sigprofiler)) {
        deletionInsertion = indels_sigprofiler2meta(indel_file = indel_sigprofiler, sample = sig_sample_name)
        meta.dt$deletionInsertion = list(as.list(deletionInsertion[["indel_fraction"]]))
        meta.dt$sigprofiler_indel_fraction = list(as.list(deletionInsertion[["indel_fraction"]]))
        meta.dt$sigprofiler_indel_count = list(as.list(deletionInsertion[["indel_count"]]))
    }

    if(!is.null(sbs_sigprofiler)) {
        signatures = sbs_sigprofiler2meta(sbs_file = sbs_sigprofiler, sample = sig_sample_name)
        meta.dt$sigprofiler_sbs_fraction = list(as.list(signatures[["sbs_fraction"]]))
        meta.dt$sigprofiler_sbs_count = list(as.list(signatures[["sbs_count"]]))
        meta.dt$signatures = list(as.list(signatures[["sbs_fraction"]])) #Changed default to sigprofiler
    }     

    ##write the json
    if(write_json) {
        message(paste0("Writing json to ",out_file))
        write_json(meta.dt,out_file,pretty = TRUE, auto_unbox=TRUE)
        if(return_table) {
            return(meta.dt)
        }
        
    } else {
        return(meta.dt)
    }
}

#' @name create_distributions
#' @title create_distributions
#' @description
#'
#' Creates the background distributions for aggregated metadata KPIs across all patients
#' 
#' @param case_reports_datadir directory containing all patient directories
#' @param filter_patients list() - subset of samples to filter on to create the distributions
#' @param cores number of cores to use for parallel processing
#' @param write_to_json boolean flag to indicate whether to write the files to JSON or not
#' @param common_dir path to the common directory in which to write the distribution JSONs (required if write_to_json is TRUE)
#' @param overwrite boolean flag to indicate whether to overwrite existing files (required if write_to_json is TRUE)
#' @return List of data tables containing distributions if write_to_json is FALSE, otherwise NULL
#' @export
create_distributions = function(
    case_reports_datadir,
    filter_patients = NULL,
    cores = 1,
    write_to_json = FALSE,
    common_dir = NULL,
    overwrite = FALSE
) {
    case_reports_data_folder = paste0(case_reports_datadir, "/")

    files <- list.files(
        case_reports_data_folder,
        pattern = "metadata.json",
        recursive = TRUE,
        full.names = TRUE
    )

    patient_ids <- basename(dirname(files))

    meta.dt <- data.table(
        meta_json = files,
        patient_id = patient_ids
    )

    meta.dt <- meta.dt[file.exists(meta_json), ]

    if (!is.null(filter_patients)) {
        meta.dt <- meta.dt[patient_id %in% filter_patients, ]
    }
    
    jsons.lst = mclapply(1:nrow(meta.dt), function(i) {
        meta.sub.dt = meta.dt[i, ]
        read_meta_data_json(meta_json = meta.sub.dt$meta_json, patient_id = meta.sub.dt$patient_id)
    }, mc.cores = cores)
    jsons.dt = rbindlist(jsons.lst, fill = TRUE)

    subset_and_setnames <- function(dt, cols, new_names) {
        if (all(cols %in% names(dt))) {
            return(dt[, ..cols] %>% setnames(., new_names))
        }
        return(NULL)
    }
    
    # add new metadata attributes here
    column_map <- list(
        snv.dt = list(
            cols = c("pair", "snv_count", "tumor_type"),
            new_names = c("pair", "value", "tumor_type")
        ),
        sv.dt = list(
            cols = c("pair", "sv_count", "tumor_type"),
            new_names = c("pair", "value", "tumor_type")
        ),
        loh.dt = list(
            cols = c("pair", "tumor_type", "loh_fraction", "loh_seglen", "loh_total_genome"),
            new_names = c("pair", "tumor_type", "value", "LOH_seg_len", "genome_width")
        ),
        ploidy.dt = list(
            cols = c("pair", "tumor_type", "ploidy", "purity"),
            new_names = c("pair", "tumor_type", "value", "purity")
        ),
        purity.dt = list(
            cols = c("pair", "tumor_type", "ploidy", "purity"),
            new_names = c("pair", "tumor_type", "ploidy", "value")
        ),
        cov_var.dt = list(
            cols = c("pair", "tumor_type", "dlrs"),
            new_names = c("pair", "tumor_type_mod", "value")
        ),
        tmb.dt = list(
            cols = c("pair", "tmb", "tumor_type"),
            new_names = c("pair", "value", "tumor_type")
        )
    )

    results <- lapply(names(column_map), function(name) {
        subset_and_setnames(jsons.dt, column_map[[name]]$cols, column_map[[name]]$new_names)
    })

    names(results) <- names(column_map)

    # signatures
    sigs.lst = convert_signature_meta_json(jsons.dt = jsons.dt, cores = cores)

    sigs_names <- c(
        "deconstruct_sigs.dt",
        "sigprofilerassignment_indels.dt",
        "sigprofilerassignment_indels_counts.dt",
        "sigprofilerassignment_sbs_counts.dt",
        "sigprofilerassignment_sbs.dt"
    )

    sigs_results <- lapply(sigs_names, function(name) {
        if (name %in% names(sigs.lst)) {
            return(sigs.lst[[name]])
        }
        return(NULL)
    })

    json.lst <- c(results, sigs_results)

    names(json.lst) <- c(
        "snvCount",
        "svCount",
        "lohFraction",
        "ploidy",
        "purity",
        "coverageVariance",
        "tmb",
        "sbs",
        "sigprofiler_indel_fraction",
        "sigprofiler_indel_count",
        "sigprofiler_sbs_count",
        "sigprofiler_sbs_fraction"
    )

    if (write_to_json) {
        if (is.null(common_dir)) {
            stop("common_dir must be provided if write_to_json is TRUE")
        }
        write_distributions_to_json(json.lst, common_dir, cores, overwrite)
        return(NULL)
    } else {
        return(json.lst)
    }
}

#' @name write_signature_jsons
#' @title write_signature_jsons
#' @description
#' Writes the different distribution JSONs for signatures from the list outputted by convert_signature_meta_json
#'
#' @param signatures_list list outputted from convert_sign
#' @param common_folder location of the signatures folder
#' @param cores cores for generating the signature distribution files
#' @return NULL
#' @export
#' @author Shihab Dider, Stanley Clarke
write_signature_jsons <- function(signatures_list, common_folder, cores = 1) {
    write_signature_json <- function(sig.dt, folder_path, sig) {
        sig.dt[, sig := sig]
        write_json(
            sig.dt,
            paste0(common_folder, folder_path, sig, ".json"),
            pretty = TRUE
        )
    }

    process_signatures <- function(sig_data, folder_path) {
        sig_add = names(sig_data) %>% grep("pair|tumor_type", ., invert = TRUE, value = TRUE)
        empty.lst = mclapply(sig_add, function(sig) {
            cols_keep = c("pair", "tumor_type", sig)
            sig.dt = sig_data[, ..cols_keep] %>% setnames(., c("pair", "tumor_type", "value"))
            write_signature_json(sig.dt, folder_path, sig)
            return(NULL)
        }, mc.cores = cores)
    }

    if ("sbs" %in% names(signatures_list)) {
        process_signatures(
            signatures_list[["sbs"]],
            "signatures/deconstructsigs_sbs_fraction/"
        )
    }
    if ("sigprofiler_indel_fraction" %in% names(signatures_list)) {
        process_signatures(
            signatures_list[["sigprofiler_indel_fraction"]],
            "signatures/sigprofiler_indel_fraction/"
        )
    }
    if ("sigprofiler_indel_count" %in% names(signatures_list)) {
        process_signatures(
            signatures_list[["sigprofiler_indel_count"]],
            "signatures/sigprofiler_indel_count/"
        )
    }
    if ("sigprofiler_sbs_count" %in% names(signatures_list)) {
        process_signatures(
            signatures_list[["sigprofiler_sbs_count"]],
            "signatures/sigprofiler_sbs_count/"
        )
    }
    if ("sigprofiler_sbs_fraction" %in% names(signatures_list)) {
        process_signatures(
            signatures_list[["sigprofiler_sbs_fraction"]],
            "signatures/sigprofiler_sbs_fraction/"
        )
        process_signatures(
            signatures_list[["sigprofiler_sbs_fraction"]],
            "signatures/sbs/"
        )
    }
}

#' @name write_distributions_to_json
#' @title write_distributions_to_json
#' @description
#'
#' Writes the distributions data table to JSON files
#' 
#' @param distributions List of data tables containing distributions
#' @param common_dir path to the common directory in which to write the distribution JSONs
#' @param cores number of cores to use for parallel processing
#' @param overwrite boolean flag to indicate whether to overwrite existing files
#' @return NULL
#' @export
write_distributions_to_json <- function(
    distributions,
    common_dir,
    cores = 1,
    overwrite = FALSE
) {
    common_folder = paste0(common_dir, "/")
    
    if(!file.exists(common_folder)) {
        stop("common_folder does not exist. Make the directory first")
    }

    signature_folder_paths = paste0(
        common_folder,
        c(
            "signatures/sbs/",
            "signatures/deconstructsigs_sbs_fraction/",
            "signatures/insertionDeletion/",
            "signatures/sigprofiler_indel_fraction/",
            "signatures/sigprofiler_indel_count/",
            "signatures/sigprofiler_sbs_count/",
            "signatures/sigprofiler_sbs_fraction/"
        )
    )

    if(!all(file.exists(signature_folder_paths))) {
        empty.lst = mclapply(signature_folder_paths, function(folder_path) {
            cmd = paste0("mkdir -p ", folder_path)
            print(paste0('Making directory ', folder_path))
            system(cmd)
            return(NULL)
        }, mc.cores = cores)
    }

    write_json_file <- function(data, file_path) {
        if (overwrite || !file.exists(file_path)) {
            write_json(data, file_path, pretty = TRUE)
        } else {
            message(paste0("File ", file_path, " already exists. Skipping."))
        }
    }

    message(paste0("writing jsons to ", common_folder))
    write_json_file(distributions$snvCount, paste0(common_folder, "/snvCount.json"))
    write_json_file(distributions$svCount, paste0(common_folder, "/svCount.json"))
    write_json_file(distributions$lohFraction, paste0(common_folder, "/lohFraction.json"))
    write_json_file(distributions$ploidy, paste0(common_folder, "/ploidy.json"))
    write_json_file(distributions$purity, paste0(common_folder, "/purity.json"))
    write_json_file(distributions$coverageVariance, paste0(common_folder, "/coverageVariance.json"))
    write_json_file(distributions$tmb, paste0(common_folder, "/tmb.json"))

    write_signature_jsons(
        signatures_list = distributions,
        common_folder = common_folder,
        cores = cores
    )
}

#' @name load_distributions
#' @title load_distributions
#' @description
#'
#' function to read in distributions files, same format returned as create_distributions with write_jsons = FALSE
#' 
#' @param common_dir path to a folder to write all 7 jsons
#' @param filter_pateints list of samples to filter on to read in the distributions
#' @return NULL
#' @export
#' @author Stanley Clarke
load_distributions = function(
    common_dir,
    filter_patients = NULL
) {
    common_dir = paste0(common_dir, "/")
    files.lst = paste0(
        common_dir,
        c(
            "coverageVariance.json",
            "lohFraction.json",
            "ploidy.json",
            "purity.json",
            "snvCount.json",
            "svCount.json",
            "tmb.json"
        )
    )
    files.dt = data.table(
        name = c(
            "coverageVariance",
            "lohFraction",
            "ploidy",
            "purity",
            "snvCount",
            "svCount",
            "tmb"
        ),
        file = files.lst
    )
    
    json.lst = lapply(setNames(nm = files.dt$name), function(name_i) {
        file_i = files.dt[name == name_i,]$file
        if(!is.null(filter_patients)) {
            json.dt = as.data.table(fromJSON(file_i))
            json.sub.dt = json.dt[pair %in% filter_patients,]
            return(json.sub.dt)
        }
        return(as.data.table(fromJSON(file_i)))
    })
    return(json.lst)
}

#' @name add_patient_to_distributions
#' @title add_patient_to_distributions
#' @description
#'
#' Adds a new patient to the existing distributions and writes the updated distributions to JSON files.
#' 
#' @param metadata_json_path Path to the new patient's metadata.json file.
#' @param common_dir Path to the common directory where the distributions are stored.
#' @param cores Number of cores to use for parallel processing.
#' @param overwrite Boolean flag to indicate whether to overwrite existing files.
#' @return Updated distributions.
#' @export
#' @author Shihab Dider
add_patient_to_distributions <- function(
    metadata_json_path,
    common_dir,
    cores = 1,
    overwrite = FALSE,
    write_to_json = FALSE
) {
    distributions <- load_distributions(common_dir)
    
    new_patient_data <- fromJSON(metadata_json_path)
    
    # Extract patient ID from the metadata.json path
    patient_id <- basename(dirname(metadata_json_path))
    
    # Update distributions with the new patient's data
    update_distribution <- function(distribution, new_data, key) {
        if (!is.null(distribution)) {
            new_entry <- data.table(
                pair = patient_id,
                value = new_data[[key]],
                tumor_type = new_data$tumor_type
            )
            return(rbind(distribution, new_entry, fill = TRUE))
        }
        return(distribution)
    }
    
    distributions$snvCount <- update_distribution(distributions$snvCount, new_patient_data, "snv_count")
    distributions$svCount <- update_distribution(distributions$svCount, new_patient_data, "sv_count")
    distributions$lohFraction <- update_distribution(distributions$lohFraction, new_patient_data, "loh_fraction")
    distributions$ploidy <- update_distribution(distributions$ploidy, new_patient_data, "ploidy")
    distributions$purity <- update_distribution(distributions$purity, new_patient_data, "purity")
    distributions$coverageVariance <- update_distribution(distributions$coverageVariance, new_patient_data, "dlrs")
    distributions$tmb <- update_distribution(distributions$tmb, new_patient_data, "tmb")
    
    # Update signature distributions
    update_signature_distribution <- function(distribution, new_data, key_prefix) {
        if (!is.null(distribution)) {
            sig_keys <- names(new_data)[grep(paste0("^", key_prefix), names(new_data))]
            for (sig_key in sig_keys) {
                new_entry <- data.table(
                    pair = patient_id,
                    value = new_data[[sig_key]],
                    tumor_type = new_data$tumor_type
                )
                distribution <- rbind(distribution, new_entry, fill = TRUE)
            }
            return(distribution)
        }
        return(distribution)
    }
    
    distributions$sbs <- update_signature_distribution(
        distributions$sbs,
        new_patient_data,
        "sbs"
    )
    distributions$sigprofiler_indel_fraction <- update_signature_distribution(
        distributions$sigprofiler_indel_fraction,
        new_patient_data,
        "sigprofiler_indel_fraction"
    )
    distributions$sigprofiler_indel_count <- update_signature_distribution(
        distributions$sigprofiler_indel_count,
        new_patient_data,
        "sigprofiler_indel_count"
    )
    distributions$sigprofiler_sbs_count <- update_signature_distribution(
        distributions$sigprofiler_sbs_count,
        new_patient_data,
        "sigprofiler_sbs_count"
    )
    distributions$sigprofiler_sbs_fraction <- update_signature_distribution(
        distributions$sigprofiler_sbs_fraction,
        new_patient_data,
        "sigprofiler_sbs_fraction"
    )
    
    # Save the updated distributions
    if (write_to_json) {
        write_distributions_to_json(distributions, common_dir, cores, overwrite)
    } else {
        print("Returning updated distributions. If you want to save them, set write_to_json = TRUE.")
        return(distributions)
    }
}

#' @name bw_temp
#' @title bw_temp
#' @description
#'
#' function to create data.table for bigwig files for pgvdb
#' 
#' @param patient_id patient.id to be added to pgvdb
#' @param order optional entry if you order plots with a column order
#' @param x path to a granges rds or a granges object
#' @param ref reference to use for pgvdb
#' @param chart_type defaultChartType for pgvdb, default is area, can also be scatterplot
#' @param visible TRUE/FALSE whether the plot is hidden or showing in pgv
#' @param title title of the plot in pgvdb
#' @param type bigwig, should not change for this plot type
#' @param field field to plot as the y value, default is foreground
#' @param overwrite TRUE/FALSE to overwrite an existing bw
#' @return NULL
#' @export
#' @author Stanley Clarke

bw_temp = function(
    patient_id = NA,
    order = NA,
    x = list(NA),
    ref = NA,
    chart_type = "area",
    visible = TRUE,
    title = NA,
    type = "bigwig",
    field = "foreground",
    overwrite = FALSE
) {
    dt1 = data.table(patient.id = patient_id,
                     visible = visible,
                     x = x,
                     type = type,
                     field = field,
                     ref = ref,
                     title = title,
                     order = order,
                     defaultChartType = chart_type,
                     overwrite = overwrite
                     )
    return(dt1)
}


#' @name arrow_temp
#' @title arrow_temp
#' @description
#'
#' function to create data.table for arrow files for pgvdb
#' 
#' @param patient_id patient.id to be added to pgvdb
#' @param order optional entry if you order plots with a column order
#' @param x path to a granges rds or a granges object
#' @param ref reference to use for pgvdb
#' @param chart_type defaultChartType for pgvdb, default is scatterplot, can also be area
#' @param visible TRUE/FALSE whether the plot is hidden or showing in pgv
#' @param title title of the plot in pgvdb
#' @param type scatterplot, should not change for this plot type
#' @param field field to plot as the y value, default is foreground
#' @param overwrite TRUE/FALSE to overwrite an existing arrow
#' @return NULL
#' @export
#' @author Stanley Clarke

arrow_temp = function(
    patient_id = NA,
    order = NA,
    x = list(NA),
    ref = NA,
    source = "coverage.arrow",
    chart_type = "scatterplot",
    visible = TRUE,
    title = NA,
    type = "scatterplot",
    field = "foreground",
    overwrite = FALSE
) {
    dt1 = data.table(patient.id = patient_id,
                     visible = visible,
                     x = x,
                     type = type,
                     field = field,
                     ref = ref,
                     source = source,
                     title = title,
                     order = order,
                     defaultChartType = chart_type,
                     overwrite = overwrite
                     )
    return(dt1)
}

#' @name genome_temp
#' @title genome_temp
#' @description
#'
#' function to create data.table for genome graphs for pgvdb
#' 
#' @param patient_id patient.id to be added to pgvdb
#' @param order optional entry if you order plots with a column order
#' @param x path to a JaBbA ggraph object or object itself as a list
#' @param ref reference to use for pgvdb
#' @param visible TRUE/FALSE whether the plot is hidden or showing in pgv
#' @param title title of the plot in pgvdb
#' @param max.cn override max cn of 100
#' @param type genome, can be changed to allelic to render allelic graphs
#' @param annotation default is list of SVs, make null if no annotations present in object
#' @param overwrite TRUE/FALSE to overwrite an existing genome json
#' @return NULL
#' @export
#' @author Stanley Clarke

genome_temp = function(
    patient_id = NA,
    order = NA,
    x = list(NA),
    ref = NA,
    source = "genome.json",
    type = "genome",
    visible = TRUE,
    title = NA,
    max.cn = NULL,
    annotation = list(c('bfb','chromoplexy','chromothripsis','del','dm','cpxdm','dup','pyrgo','rigma','simple','tic','tyfonas')),
    overwrite = FALSE
) {
                                        #use type = allelic to make a color a genome graph
    dt1 = data.table(
        patient.id = patient_id,
        type = type,
        visible = visible,
        title = title,
        x = x,
        ref = ref,
        source = source,
        max.cn = max.cn,
        order = order,
        annotation = annotation,
        overwrite = overwrite
    )
    return(dt1)
}

#' @name walks_temp
#' @title walks_temp
#' @description
#'
#' function to create data.table for walks for pgvdb
#' 
#' @param patient_id patient.id to be added to pgvdb
#' @param order optional entry if you order plots with a column order
#' @param x walks object as a list
#' @param ref reference to use for pgvdb
#' @param visible TRUE/FALSE whether the plot is hidden or showing in pgv
#' @param title title of the plot in pgvdb
#' @param type walk, do not change for this plot type
#' @param annotation default is list of SVs, make null if no annotations present in object
#' @param overwrite TRUE/FALSE to overwrite an existing genome json
#' @param tag optional argument, can be binset to override y spacing in pgv
#' @return NULL
#' @export
#' @author Stanley Clarke

walks_temp = function(
    patient_id = NA,
    order = NA,
    x = list(NA),
    ref = NA,
    source = "walks.json",
    type = "walk",
    visible = TRUE,
    title = NA,
    tag = NA,
    overwrite = FALSE
) {
    dt1 = data.table(
        patient.id = patient_id,
        visible = visible,
        x = x,
        type = type,
        order = order,
        ref = ref,
        source = source,
        title = title,
        overwrite = overwrite
    )
    return(dt1)
}

#' @name mutations_temp
#' @title mutations_temp
#' @description
#'
#' function to create data.table for mutations for pgvdb
#' 
#' @param patient_id patient.id to be added to pgvdb
#' @param order optional entry if you order plots with a column order
#' @param x mutations object as a list
#' @param ref reference to use for pgvdb
#' @param visible TRUE/FALSE whether the plot is hidden or showing in pgv
#' @param title title of the plot in pgvdb
#' @param type walk, do not change for this plot type
#' @param annotation default is list of SVs, make null if no annotations present in object
#' @param overwrite TRUE/FALSE to overwrite an existing genome json
#' @param tag optional argument, can be binset to override y spacing in pgv
#' @return NULL
#' @export
#' @author Stanley Clarke

mutations_temp = function(
    patient_id = NA,
    order = NA,
    x = list(NA),
    ref = NA,
    source = "mutations.json",
    field,
    type = "mutations",
    visible = TRUE,
    title = NA,
    tag = NA,
    overwrite = FALSE
) {
    dt1 = data.table(
        patient.id = patient_id,
        visible = visible,
        type = type,
        x = x,
        field = field,
        order = order,
        ref = ref,
        source = source,
        title = title,
        overwrite = overwrite
    )
    return(dt1)
}

#' @name ppfit_temp
#' @title ppfit_temp
#' @description
#'
#' function to create data.table for ppfit for pgvdb
#' 
#' @param patient_id patient.id to be added to pgvdb
#' @param order optional entry if you order plots with a column order
#' @param x jabba.rds
#' @param ref reference to use for pgvdb
#' @param visible TRUE/FALSE whether the plot is hidden or showing in pgv
#' @param title title of the plot in pgvdb
#' @param type ppfit, do not change for this plot type
#' @param annotation default is NULL, currently won't work for ppfit
#' @param overwrite TRUE/FALSE to overwrite an existing genome json
#' @param tag optional argument, can be binset to override y spacing in pgv
#' @return NULL
#' @export
#' @author Stanley Clarke

ppfit_temp = function(
    patient_id = NA,
    order = NA,
    x = list(NA),
    ref = NA,
    source = "ppfit.json",
    type = "ppfit",     #use type = allelic to make a colored genome graph
    visible = TRUE,
    title = NA,
    annotation = NULL,
    overwrite = FALSE
) {
    
    dt1 = data.table(
        patient.id = patient_id,
        type = type,
        visible = visible,
        title = title,
        x = x,
        ref = ref,
        source = source,
        order = order,
        annotation = annotation,
        overwrite = overwrite
    )
    return(dt1)
}

#' @name create_ppfit_json
#' @title create_ppfit_json
#' @description
#'
#' function to create segmentation plots in case reports
#' 
#' @param balanced_gg the balanced_gg ggraph or the path to it
#' @param cov path to associated coverage, if null, will try to pull path from pairs table
#' @param path_obj path to flow directory, if null, must supply coverage path
#' @param settings_json path to settings.json (from PGV)
#' @param ref reference name
#' @param max_na max.na used with JaBbA
#' @param out_file location to write json
#' @param write_json TRUE/FALSE whether to write the json
#' @param return_table TRUE/FALSE whether to return the data
#' @param overwrite TRUE/FALSE whether to overwrite existing file
#' @param cores cores for JaBbA:::segstats
#' @return NULL or segstats table
#' @export
#' @author Stanley Clarke, Tanubrata Dey

create_ppfit_json = function(
    balanced_gg,
    cov_path = NULL,
    path_obj = NULL,
    settings_json,
    ref = NULL,
    max_na = NULL,
    out_file = NULL,
    write_json = TRUE,
    overwrite = FALSE,
    return_table = FALSE,
    cores = 1
) {

    if(!is.null(out_file)) {
        if(!overwrite) {
            if(file.exists(out_file)) {
                print('Output already exists! - skipping sample')
                return(NA)
            }
        }
    }

    #'drcln_cov = readRDS(thisp$decomposed_cov[i])
    ##make this work with complex where the cov file was not an input and with jabba_gg
    ## x = path_obj %>% sniff %>% inputs %>% select(CovFile, maxna) #get coverage that was used for the jabba run
    if (!is.null(cov_path)) {
        cov = readRDS(cov_path)
    } else if (!is.null(path_obj)) {
        inputs.dt = path_obj %>% sniff %>% inputs
        if(!any(grepl("CovFile", names(inputs.dt))) & any(grepl("jabba", names(inputs.dt)))) {
            x = inputs.dt$jabba %>% sniff %>% inputs %>% .[,.(CovFile,maxna)]
        } else if (!any(grepl("CovFile", names(inputs.dt))) & any(grepl("jab", names(inputs.dt)))) {
            x = inputs.dt$jab %>% sniff %>% inputs %>% .[,.(CovFile,maxna)]
        } else {
            x = path_obj %>% sniff %>% inputs %>% .[,.(CovFile,maxna)]
        }
        cov = readRDS(x$CovFile)
        max_na = as.numeric(x$maxna)
    } else {
        stop("Must supply either coverage path or flow directory path object")
    }

    if (is.null(ref)) {
        warning("ref was not passed, setting to default: hg19")
        ref = "hg19"
    }

    if (is.null(max_na)) {
        warning("max_na was not passed/found, setting to default: 0.9")
        max_na = 0.9
    }

    if ("ratio" %in% names(mcols(cov))) {
        message(paste0("Raw 'cov.rds' was used as input for JaBbA ",path_obj, ", will consider field as 'ratio''\n"))
        field = "ratio"
    } else if ("foreground" %in% names(mcols(cov))) {
        message(paste0("Drycleaned 'drycleaned.cov.rds' was used as input for JaBbA ",path_obj, ", will consider field as 'foreground''\n"))
        field = "foreground"
    }
    if(!(field %in% c("ratio","foreground"))) {
        stop("Cov file is not clear. Ratio nor foreground in the the columns of the coverage file")
    }
    ##need to replace NaN with NA or JaBbA:::segstats breaks
    if(field == "ratio") {
        cov$ratio = gsub("NaN",NA,cov$ratio) %>% as.numeric
    } else if (field == "foreground") {
        cov$foreground = gsub("NaN",NA,cov$foreground) %>% as.numeric
    }
    if (is(balanced_gg, "gGraph")) {
        balanced_gg_gr = balanced_gg$nodes$gr
    } else if (is(balanced_gg, "character")) {
        balanced_gg_gr = readRDS(balanced_gg)$nodes$gr
    }
    
    segstats = JaBbA:::segstats(balanced_gg_gr,
                                cov,
                                field = field,
                                prior_weight = 1,
                                max.chunk = 1e8,
                                ## subsample = subsample,
                                mc.cores = cores,
                                verbose = FALSE,
                                max.na = max_na,
                                lp = FALSE)
    segstats.dt = gr2dt(segstats)
    names(segstats.dt) = gsub("\\.","_",names(segstats.dt))
    if(write_json) {
        message("Cleaning up for writing ppfit to json")
        seq_lengths <- gGnome::parse.js.seqlengths(
                                 settings_json,
                                 js.type = "PGV",
                                 ref = ref
                               )
        segstats.gr = GRanges(segstats.dt, seqlengths = seq_lengths) %>% trim
        ggraph = readRDS(balanced_gg)
        ggraph2 = gG(nodes = segstats.gr, edges = ggraph$edges$dt)
        fields.keep = names(segstats.dt) %>% grep("cn",.,invert = TRUE, value = TRUE)
        ggraph2$set(y.field = "cn")
        ## check for overlap in sequence names
        ggraph.reduced <- ggraph2[seqnames %in% names(seq_lengths)]
        if (length(ggraph.reduced) == 0) {
          stop(sprintf(
            'There is no overlap between the sequence names in the reference
                    used by PGV and the sequences in your gGraph. Here is an
                    example sequence from your gGraph: "%s". And here is an
                    example sequence from the reference used by gGnome.js: "%s"',
            seqlevels(ggraph.reduced$nodes$gr)[1], names(seq_lengths)[1]
          ))
        }
        message(paste0("Writing json to ",out_file))
        gGnome::refresh(ggraph.reduced)$json(
            filename = out_file,
            verbose = TRUE,
            maxcn = 500,
            nfields = fields.keep,
            save = TRUE
        )
    }
    if(return_table) {
        return(segstats.dt)
    }
}


#' @name cov_abs
#' @title cov_abs
#' @description
#'
#' function to run rel2abs on coverage taking either a ggraph or purity & ploidy
#' 
#' @param dryclean_cov dryclean coverage
#' @param jabba_gg optional jabba ggraph. If null, needs purity or ploidy
#' @param purity optional purity value. If null needs ggraph
#' @param ploidy optional ploidy value. If null needs ggraph
#' @param field column in granges to convert with rel2abs
#' @param new_col new column to add to add for the converted rel2abs
#' @return NULL or segstats table
#' @export
#' @author Stanley Clarke
cov2abs = function(
    dryclean_cov,
    jabba_gg = NULL,
    purity = NULL,
    ploidy = NULL,
    field = "foreground",
    new_col = "foregroundabs"
) {
    cov_gr = readRDS(dryclean_cov)
    if(!is.null(jabba_gg)) {
        gg = readRDS(jabba_gg)
        purity = gg$meta$purity
        ploidy = gg$meta$ploidy
    }
    if(!is.null(purity) && !is.null(ploidy)) {
        purity = purity
        ploidy = ploidy
    }
    mcols(cov_gr)[new_col] = rel2abs(gr = cov_gr,
                                     purity = purity,
                                     ploidy = ploidy,
                                     field = field
                                     )
    return(cov_gr)
}

#' @name cov2arrow_pgv
#' @title cov2arrow_pgv
#' @description
#'
#' function to create arrow with rel2abs from coverage file. Can take a ggraph or specified purity and ploidy to calculate rel2abs
#' 
#' @param patient.id patient name to add to pgv
#' @param dryclean_cov dryclean coverage
#' @param ref reference for pgv
#' @param field column in granges to convert with rel2abs
#' @param jabba_gg optional jabba ggraph. If null, needs purity or ploidy
#' @param purity optional purity value. If null needs ggraph
#' @param ploidy optional ploidy value. If null needs ggraph
#' @param title title for ggraph
#' @param mask optional mask to use after rebinning
#' @param title title for ggraph
#' @param ref reference to use for pgvdb
#' @param title optional title for plot for pgvdb
#' @param seq.fix optional seqlengths vector to fix granges seqlengths
#' @param chart_type default chart type for plot in pgvdb. scatterplot or area
#' @param visible whether the plot is visible in pgv
#' @param field column to do rel2abs on
#' @param new_col new column after rel2abs
#' @param overwrite whether to overwrite the current bigwig
#' @param order optional order if using a column order in your pgvdb object to sort
#' @param binsize size to rebin coverages, default 1e4
#' @return NULL or segstats table
#' @export
#' @author Stanley Clarke
cov2arrow_pgv = function(
    patient.id,
    dryclean_cov,
    jabba_gg = NULL,
    purity = NULL,
    ploidy = NULL,
    mask = NULL,
    ref,
    title = NA,
    seq.fix = NULL,
    chart_type = "scatterplot",
    visible = TRUE,
    field = "foreground",
    new_col = "foregroundabs",
    overwrite = FALSE,
    order = NA,
    binsize = 1e4
) {
    if(!is.null(jabba_gg)) {
        cov_gr = cov2abs(dryclean_cov, jabba_gg, field = field, new_col = new_col)
    } else if(!is.null(purity) && !is.null(ploidy)) {
        cov_gr = cov2abs(dryclean_cov, purity = purity, ploidy = ploidy)
    } else {
        cov_gr = readRDS(dryclean_cov)
    }
    if(!is.null(mask)) {
        cov_gr = gr.val(cov_gr,mask, "mask")
        cov_gr = cov_gr %Q% (is.na(mask))
        cov_gr$mask = NULL
    }
    cov_gr2 = rebin(cov_gr, binsize, field = field)
    if(length(cov_gr2$foregroundabs[cov_gr2$foregroundabs < 0]) > 0) {
        cov_gr2$foregroundabs[cov_gr2$foregroundabs < 0] = 0
    }    
    add.dt = arrow_temp(patient_id = patient.id, ref = ref, field = field, x = list(cov_gr2), title = title, overwrite = overwrite, order = NA, chart_type = chart_type, visible = visible)
    return(add.dt)
}

#' @name cov2bw_pgv
#' @title cov2bw_pgv
#' @description
#'
#' function to create bigwig with rel2abs from coverage file. Can take a ggraph or specified purity and ploidy to calculate rel2abs
#' 
#' @param patient.id patient name to add to pgv
#' @param dryclean_cov dryclean coverage
#' @param ref reference for pgv
#' @param field column in granges to convert with rel2abs
#' @param jabba_gg optional jabba ggraph. If null, needs purity or ploidy
#' @param purity optional purity value. If null needs ggraph
#' @param ploidy optional ploidy value. If null needs ggraph
#' @param mask mask to remove coverages from
#' @param title title for ggraph
#' @param ref reference to use for pgvdb
#' @param title optional title for plot for pgvdb
#' @param seq.fix optional seqlengths vector to fix granges seqlengths
#' @param chart_type default chart type for plot in pgvdb. scatterplot or area
#' @param visible whether the plot is visible in pgv
#' @param field column to do rel2abs on
#' @param new_col new column after rel2abs
#' @param overwrite whether to overwrite the current bigwig
#' @param order optional order if using a column order in your pgvdb object to sort
#' @param mask optional mask to use after rebinning
#' @return NULL or segstats table
#' @export
#' @author Stanley Clarke

## function to not rebin using higlass but mask
cov2bw_pgv = function(
    patient.id,
    dryclean_cov,
    jabba_gg = NULL,
    purity = NULL,
    ploidy = NULL,
    mask = NULL,
    ref,
    title = NA,
    seq.fix = NULL,
    chart_type = "scatterplot",
    visible = TRUE,
    field = "foreground",
    new_col = "foregroundabs",
    overwrite = FALSE,
    order = NA
) {
    if(!is.null(jabba_gg)) {
        cov_gr = cov2abs(dryclean_cov, jabba_gg, field = field, new_col = new_col)
    } else if(!is.null(purity) && !is.null(ploidy)) {
        cov_gr = cov2abs(dryclean_cov, purity = purity, ploidy = ploidy)
    } else {
        cov_gr = readRDS(dryclean_cov)
    }
    ## cov_gr = cov2abs(dryclean_cov, jabba_gg, field = field, new_col = new_col)
    if(!is.null(mask)) {
        cov_gr = gr.val(cov_gr,mask, "mask")
        cov_gr = cov_gr %Q% (is.na(mask))
        cov_gr$mask = NULL
    }
    if(length(cov_gr$foregroundabs[cov_gr$foregroundabs < 0]) > 0) {
        cov_gr$foregroundabs[cov_gr$foregroundabs < 0] = 0
    }
    if(!is.null(seq.fix)) {
        ## fix seqlengths to specified seqlengths
        cov_gr = GRanges(as.data.table(cov_gr),seqlengths = seq.fix) %>% trim()
    }
    add.dt = bw_temp(
        patient_id = patient.id,
        ref = ref,
        field = new_col,
        x = list(cov_gr),
        title = title,
        overwrite = overwrite,
        order = NA,
        chart_type = chart_type,
        visible = visible
    )
    return(add.dt)
}

#' @name indels_sigprofiler2meta
#' @title indels_sigprofiler2meta
#' @description
#' function to generate metadata indel signature list from sigprofiler output
#'
#' @param indel_file path to Assignment_Solution_Activities.txt, output of sigprofiler
#' @param sample sample or pair name-needed to subset to the correct sample
#' 
#' @return 
#' @export
#' @author Sukanya Panja, Stanley Clarke
indels_sigprofiler2meta = function(indel_file, sample) {
    ## indel signatures first
    indel_sig = fread(indel_file)
    indel_sig[, pair := gsub("_somatic","",Samples)]
    indel_sig[,Samples := NULL]
    indel_sig = indel_sig[pair == sample,]
    indel_sig_avg = copy(indel_sig)
    indel_sig_avg[, pair := NULL]
    row_sum = indel_sig_avg %>% rowSums()
    indels.dt = melt.data.table(indel_sig_avg, measure.vars = names(indel_sig_avg)) %>% setnames(.,c("signature","value"))
    indels.dt[, avg_value := value / row_sum]
    indels.vect = indels.dt$avg_value
    names(indels.vect) = indels.dt$signature
    indels_counts.vect = indels.dt$value
    names(indels_counts.vect) = indels.dt$signature
    indels.lst = list(indels.vect,indels_counts.vect)
    names(indels.lst) = c("indel_fraction","indel_count")
    return(indels.lst)
}


#' @name sbs_deconstructSigs2meta
#' @title sbs_deconstructSigs2meta
#' @description
#' generates list of SBS signatures from the output of deconstruct sigs to add to metadata
#'
#' @param sbs_file path to rds outpyut from deconstructSigs
#' @param sample sample or pair name-needed to subset to the correct sample
#' 
#' @return 
#' @export
#' @author Sukanya Panja, Stanley Clarke
sbs_deconstructSigs2meta = function(sbs_file, sample) {
    sig_file = readRDS(sbs_file)
    weights = as.data.table(sig_file$weights)
    weights = t(weights)
    sigs.dt = as.data.table(weights, keep.rownames = "Signature") %>% setnames(.,c("Signature","weights"))
    sigs.dt[, Signature := gsub("Signature.","SBS", Signature)]
    sigs.vect = sigs.dt$weights
    names(sigs.vect) = sigs.dt$Signature
    ##pair.vect = sample
    ##names(pair.vect) = "pair"
    ##sigs.vect = c(pair.vect,sigs.vect)
    ## signatures = list(signatures = as.list(sigs.vect))
    return(sigs.vect)
}


#' @name sbs_sigprofiler2meta
#' @title sbs_sigprofiler2meta
#' @description
#' generates list of SBS signature counts and percentages from the output of deconstruct sigs to add to metadata
#'
#' @param sbs_file path to Assignment_Solution_Activities.txt, output of sigprofiler
#' @param sample sample or pair name-needed to subset to the correct sample
#' 
#' @return list of length 2, first being the signatures as a fraction and second being signatures as counts
#' @export
#' @author Stanley Clarke
sbs_sigprofiler2meta = function(sbs_file, sample) {
    sig.dt = fread(sbs_file)
    sig.dt[, pair := gsub("_somatic","",Samples)]
    sig.dt = sig.dt[pair == sample,]
    sig.dt[,Samples := NULL]
    sig.dt_avg = copy(sig.dt)
    sig.dt_avg[, pair := NULL]
    row_sum = sig.dt_avg %>% rowSums()
    sigs.dt = melt.data.table(sig.dt_avg, measure.vars = names(sig.dt_avg)) %>% setnames(.,c("signature","value"))
    sigs.dt[, avg_value := value / row_sum]
    sigs.vect = sigs.dt$avg_value
    names(sigs.vect) = sigs.dt$signature
    sigs_counts.vect = sigs.dt$value
    names(sigs_counts.vect) = sigs.dt$signature
    sigs.lst = list(sigs.vect,sigs_counts.vect)
    names(sigs.lst) = c("sbs_fraction","sbs_count")
    return(sigs.lst)
}


#' @name read_meta_data_json
#' @title read_meta_data_json
#' @description
#' Reads in meta data jsons and gets signatures that are lists into the correct list format in the data.table. Used in create_distributions
#'
#' @param meta_json path to metadata.json for case reports
#' @param patient_id sample name for metadata_json
#' 
#' @return data.table of the metadata.json for each sample
#' @export
#' @author Stanley Clarke
read_meta_data_json = function(meta_json, patient_id) {
    json.dt = as.data.table(jsonlite::read_json(meta_json,simplifyVector = TRUE)) ## have to change from this to work with signatures
    keep_cols = names(json.dt) %>% grep("signatures|deletionInsertion|sigprofiler|deconstructsigs",., value = TRUE, invert = TRUE)
    json.dt = json.dt[,..keep_cols]
    json.lst = jsonlite::read_json(meta_json,simplifyVector = FALSE)
    if(!is.null(json.lst[[1]]$signatures)) {
        json.dt$signatures = list(json.lst[[1]]$signatures)
    }
    if(!is.null(json.lst[[1]]$deletionInsertion)) {
        json.dt$deletionInsertion = list(json.lst[[1]]$deletionInsertion)
    }
    if(!is.null(json.lst[[1]]$deconstructsigs_sbs_fraction)) {
        json.dt$deconstructsigs_sbs_fraction = list(json.lst[[1]]$deconstructsigs_sbs_fraction)
    }
    if(!is.null(json.lst[[1]]$sigprofiler_indel_fraction)) {
        json.dt$sigprofiler_indel_fraction = list(json.lst[[1]]$sigprofiler_indel_fraction)
    }
    if(!is.null(json.lst[[1]]$sigprofiler_indel_count)) {
        json.dt$sigprofiler_indel_count = list(json.lst[[1]]$sigprofiler_indel_count)
    }
    if(!is.null(json.lst[[1]]$sigprofiler_sbs_fraction)) {
        json.dt$sigprofiler_sbs_fraction = list(json.lst[[1]]$sigprofiler_sbs_fraction)
    }
    if(!is.null(json.lst[[1]]$sigprofiler_sbs_count)) {
        json.dt$sigprofiler_sbs_count = list(json.lst[[1]]$sigprofiler_sbs_count)
    }
    return(json.dt)
}

#' @name convert_signature_meta_json
#' @title convert_signature_meta_json
#' @description
#' converts signatures from the list objects in the meta data to data.tables to write jsons for distributions. Used in create_distributions
#'
#' @param jsons.dt json dt after using read_meta_data_json
#' @param cores cores for generating each signature
#' @return list object with all of the signature distribution data.tables
#' @export
#' @author Stanley Clarke
convert_signature_meta_json = function(jsons.dt, cores = 1) {
    ## signatures-sbs deconstruct sigs ## in column signatures but also in deconstructsigs_sbs_fraction
    if("signatures" %in% names(jsons.dt)) {
        jsons.dt2 = copy(jsons.dt)
        jsons.dt2[, length_signatures := sapply(signatures, function(x) length(unlist(x)))]
        jsons.dt2 = jsons.dt2[length_signatures != 0,]
        sigs.lst = mclapply(1:nrow(jsons.dt2), function(x) {
            jsons.sub.dt2 = jsons.dt2[x,]
            sigs.dt = jsons.sub.dt2$signatures[[1]] %>% as.data.table()
            sigs.dt[, pair := jsons.sub.dt2$pair]
            sigs.dt[, tumor_type := jsons.sub.dt2$tumor_type]
            return(sigs.dt)
        }, mc.cores = cores)
        deconstruct_sigs.dt = rbindlist(sigs.lst, fill = TRUE)
    }
    ## signatures-indels sigprofiler assignment## in column deletionInsertion and sigprofiler_indel_fraction
    if("deletionInsertion" %in% names(jsons.dt)) {
        jsons.dt2 = copy(jsons.dt)
        jsons.dt2[, length_signatures := sapply(deletionInsertion, function(x) length(unlist(x)))]
        jsons.dt2 = jsons.dt2[length_signatures != 0,]
        sigs.lst = mclapply(1:nrow(jsons.dt2), function(x) {
            jsons.sub.dt2 = jsons.dt2[x,]
            sigs.dt = jsons.sub.dt2$deletionInsertion[[1]] %>% as.data.table()
            sigs.dt[, pair := jsons.sub.dt2$pair]
            sigs.dt[, tumor_type := jsons.sub.dt2$tumor_type]
            return(sigs.dt)
        }, mc.cores = cores)
        sigprofilerassignment_indels.dt = rbindlist(sigs.lst, fill = TRUE)
    }
    ## signatures-indel counts sigprofiler assignment
    if("sigprofiler_indel_count" %in% names(jsons.dt)) {
        jsons.dt2 = copy(jsons.dt)
        jsons.dt2[, length_signatures := sapply(sigprofiler_indel_count, function(x) length(unlist(x)))]
        jsons.dt2 = jsons.dt2[length_signatures != 0,]
        sigs.lst = mclapply(1:nrow(jsons.dt2), function(x) {
            jsons.sub.dt2 = jsons.dt2[x,]
            sigs.dt = jsons.sub.dt2$sigprofiler_indel_count[[1]] %>% as.data.table()
            sigs.dt[, pair := jsons.sub.dt2$pair]
            sigs.dt[, tumor_type := jsons.sub.dt2$tumor_type]
            return(sigs.dt)
        }, mc.cores = cores)
        sigprofilerassignment_indels_counts.dt = rbindlist(sigs.lst, fill = TRUE)
    } else {
        sigprofilerassignment_indels_counts.dt = NULL
    }
    ## signatures-sigprofiler sbs fraction
    if("sigprofiler_sbs_fraction"  %in% names(jsons.dt)) {
        jsons.dt2 = copy(jsons.dt)
        jsons.dt2[, length_signatures := sapply(sigprofiler_sbs_fraction, function(x) length(unlist(x)))]
        jsons.dt2 = jsons.dt2[length_signatures != 0,]
        sigs.lst = mclapply(1:nrow(jsons.dt2), function(x) {
            jsons.sub.dt2 = jsons.dt2[x,]
            sigs.dt = jsons.sub.dt2$sigprofiler_sbs_fraction[[1]] %>% as.data.table()
            sigs.dt[, pair := jsons.sub.dt2$pair]
            sigs.dt[, tumor_type := jsons.sub.dt2$tumor_type]
            return(sigs.dt)
        }, mc.cores = cores)
        sigprofilerassignment_sbs.dt = rbindlist(sigs.lst, fill = TRUE)
    } else {
        sigprofilerassignment_sbs.dt = NULL
    }
    
    ## signatures-sigprofiler sbs count
    if("sigprofiler_sbs_count"  %in% names(jsons.dt)) {
        jsons.dt2 = copy(jsons.dt)
        jsons.dt2[, length_signatures := sapply(sigprofiler_sbs_count, function(x) length(unlist(x)))]
        jsons.dt2 = jsons.dt2[length_signatures != 0,]
        sigs.lst = mclapply(1:nrow(jsons.dt2), function(x) {
            jsons.sub.dt2 = jsons.dt2[x,]
            sigs.dt = jsons.sub.dt2$sigprofiler_sbs_count[[1]] %>% as.data.table()
            sigs.dt[, pair := jsons.sub.dt2$pair]
            sigs.dt[, tumor_type := jsons.sub.dt2$tumor_type]
            return(sigs.dt)
        }, mc.cores = cores)
        sigprofilerassignment_sbs_count.dt = rbindlist(sigs.lst, fill = TRUE)
    } else {
        sigprofilerassignment_sbs_count.dt = NULL
    }
    ## create list object to return
    list_return = list(deconstruct_sigs.dt, sigprofilerassignment_indels.dt, sigprofilerassignment_indels_counts.dt, sigprofilerassignment_sbs.dt, sigprofilerassignment_sbs_count.dt)
    names(list_return) = c("deconstruct_sigs.dt", "sigprofilerassignment_indels.dt", "sigprofilerassignment_indels_counts.dt", "sigprofilerassignment_sbs.dt", "sigprofilerassignment_sbs_counts.dt")
    return(list_return)
}
